{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffbe9b3-80bb-4fc4-bebb-974345c1c418",
   "metadata": {},
   "source": [
    "# **Build a Tool Calling Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742cdfd-e7e7-4954-b26a-e095a033b6ba",
   "metadata": {},
   "source": [
    "Estimated time needed: **1** hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b90a87-769d-4735-9c56-1a3bdb0445f3",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can interact with external systems. You'll learn how to create custom tools that enable an LLM to perform specific actions, from extracting video IDs to fetching YouTube transcripts and metadata. Through hands-on examples, you'll first implement manual tool calling to understand the underlying mechanics, then build a flexible YouTube interaction system that can search videos, extract transcripts, fetch trending content, and generate summaries. By the end of this lab, you'll understand how to construct both fixed-sequence and recursive tool-calling chains, allowing your AI assistants to dynamically decide which tools to use and when to use them, creating truly intelligent agents that can reason about and interact with the world around them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc7524-71de-403c-ab55-37216366ff3b",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "   <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "   <li>\n",
    "       <a href=\"#Setup\">Setup</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "           <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Tools\">Tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-video-ID-extraction-tool\">Defining video ID extraction tool</a></li>\n",
    "           <li><a href=\"#Tool-list\">Tool list</a></li>\n",
    "           <li><a href=\"#Defining-transcript-fetching-tool\">Defining transcript fetching tool</a></li>\n",
    "           <li><a href=\"#Defining-YouTube-search-tool\">Defining YouTube search tool</a></li>\n",
    "           <li><a href=\"#Defining-metadata-extraction-tool\">Defining metadata extraction tool</a></li>\n",
    "           <li><a href=\"#Defining-trending-videos-tool\">Defining trending videos tool</a></li>\n",
    "           <li><a href=\"#Defining-thumbnail-retrieval-tool\">Defining thumbnail retrieval tool</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Binding-tools\">Binding tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#How-the-LLM-calls-a-tool\">How the LLM calls a tool</a></li>\n",
    "           <li><a href=\"#LangChain-tool-binding-process\">LangChain tool binding process</a></li>\n",
    "           <li><a href=\"#Extracting-tool-call-information\">Extracting tool call information</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Automating-the-tool-calling-process\">Automating the tool calling process</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Building-the-summarization-chain\">Building the summarization chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Recursive-chain-flow\">Recursive chain flow</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-the-core-processing-logic\">Defining the core processing logic</a></li>\n",
    "           <li><a href=\"#Building-the-complete-universal-chain\">Building the complete universal chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "</ol>\n",
    "\n",
    "<li><a href=\"#Exercise\">Exercise</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979eba6e-ac3b-4394-8fba-f0bce643af32",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Create custom tools that extend the capabilities of language models\n",
    "- Build both manual and automated tool calling chains\n",
    "- Implement recursive tool calling for dynamic, multi-step operations\n",
    "- Develop AI agents that can interact with YouTube's content programmatically\n",
    "- Apply tool calling techniques to extract, process, and summarize information from external sources\n",
    "- Design flexible workflows that allow LLMs to reason about when and how to use available tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0cb3b-dc32-4b9d-ad91-e909194b4f67",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240a1c0-5a12-415b-976e-fe5f31adff84",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bce72-29ec-4ced-83c5-4c43957a0318",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`pytube`](https://pytube.io/en/latest/) for accessing YouTube videos and their metadata programmatically.\n",
    "*   [`youtube-transcript-api`](https://github.com/jdepoix/youtube-transcript-api) for fetching transcripts from YouTube videos.\n",
    "*   [`langchain`](https://python.langchain.com/docs/get_started/introduction) for building tool-enabled LLM applications.\n",
    "*   [`langchain-community`](https://python.langchain.com/docs/integrations/providers/) for additional LangChain integrations.\n",
    "*   [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for connecting to OpenAI's language models.\n",
    "*   [`yt-dlp`](https://github.com/yt-dlp/yt-dlp) for enhanced YouTube data extraction capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf41917-c170-45ec-890a-4e2e1e661557",
   "metadata": {},
   "source": [
    "### Installing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c020d79-86de-4f43-bd0d-fcc586741786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install pytube \n",
    "# %pip install youtube-transcript-api==1.1.0\n",
    "# %pip install langchain-community==0.3.16\n",
    "# %pip install langchain==0.3.23\n",
    "# %pip install langchain-openai==0.3.14\n",
    "# %pip install yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c73f8-bb48-4747-9bee-bcf1a00aca19",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17581378-b337-417a-a7ad-08806d4b1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pytube import YouTube\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import display, JSON\n",
    "import yt_dlp\n",
    "from typing import List, Dict\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress pytube errors\n",
    "import logging\n",
    "pytube_logger = logging.getLogger('pytube')\n",
    "pytube_logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress yt-dlp warnings\n",
    "yt_dpl_logger = logging.getLogger('yt_dlp')\n",
    "yt_dpl_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33adc8b-d0d7-4127-8e21-8cd421032f00",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb88dc3-32b7-4b5b-819e-04034c311185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachneet/projects/agents_experimental/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"qwen/qwen3-32b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user wrote \"Hello, world!\". That\\'s a classic first line in programming. They might be testing the waters or starting a conversation. I should respond warmly and invite them to ask questions or share what\\'s on their mind. Let me keep it friendly and open-ended.\\n\\nHmm, maybe they want to talk about programming since \"Hello, world!\" is often the first program. Or maybe they just wanted to greet me. Either way, I should acknowledge their greeting and offer assistance. Something like, \"Hello! How can I assist you today?\" That should work.\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 12, 'total_tokens': 144, 'completion_time': 0.320312713, 'prompt_time': 0.000482822, 'queue_time': 0.089944676, 'total_time': 0.320795535}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6ddb69fc-d985-4fb4-ae84-25b839c95c7e-0', usage_metadata={'input_tokens': 12, 'output_tokens': 132, 'total_tokens': 144})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test LLM\n",
    "llm.invoke([HumanMessage(content=\"Hello, world!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896aa1dc-a31d-4beb-a8aa-296ea9c01ae5",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API key. This lab uses the `init_chat_model` function from `langchain`. To use the model you must set the environment variable `OPENAI_API_KEY` to your OpenAI API key. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e28d807-f681-419c-9765-c3f3888cd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48829a30-4392-49a4-8649-0678f7125235",
   "metadata": {},
   "source": [
    "# Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1c6cf-37bc-4455-8210-4e7132af8a84",
   "metadata": {},
   "source": [
    "\n",
    "## Creating custom tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blooks a  tool, consider the following tools:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "\n",
    "## Key components\n",
    "\n",
    "1. **@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "2. **Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "3. **Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "4. **Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Critical for tool selection by the LLM\n",
    "\n",
    "5. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n",
    "\n",
    "\n",
    "### Defining video ID extraction tool\n",
    "\n",
    "Now you'll define a function `extract_video_id` by denoting it as a tool that will help you to extract the video ID from a given URL. This is necessary because many YouTube API operations, including transcript extraction, require the video ID rather than the complete URL. The function uses regular expressions to handle different YouTube URL formats (standard, shortened, and embedded) and extract the 11-character video ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad69d8d-d799-46f1-95f4-ee5cfbf61a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the 11-character YouTube video ID from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): A YouTube URL containing a video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted video ID or error message if parsing fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex pattern to match video IDs\n",
    "    pattern = r'(?:v=|be/|embed/)([a-zA-Z0-9_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else \"Error: Invalid YouTube URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205e873-701e-4076-8921-d378b9d63465",
   "metadata": {},
   "source": [
    "The decorator wraps your function, adding those attributes (.name, .description, .func) and registering it with LangChain's tool system. The original function becomes accessible through the .func attribute, but the overall object is an instance of LangChain's tool class, with additional methods like .run() for direct invocation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Testing the video ID extraction tool\n",
    "\n",
    "\n",
    "Now you'll be testing your `extract_video_id` tool to verify that it's correctly registered with LangChain. These print statements will show you:\n",
    "1. The tool's name (as it will be referenced by the LLM)\n",
    "2. The tool's description (which helps the LLM understand when to use this tool)\n",
    "3. The actual function reference that will be called\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eeb2ac8-ae38-48d6-89b2-7b2b6df17493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n",
      "----------------------------\n",
      "Extracts the 11-character YouTube video ID from a URL.\n",
      "\n",
      "Args:\n",
      "    url (str): A YouTube URL containing a video ID.\n",
      "\n",
      "Returns:\n",
      "    str: Extracted video ID or error message if parsing fails.\n",
      "----------------------------\n",
      "<function extract_video_id at 0x1352985e0>\n"
     ]
    }
   ],
   "source": [
    "print(extract_video_id.name)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.description)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a944a56-770c-4e5b-8f08-9943db7e81c8",
   "metadata": {},
   "source": [
    "#### Testing tool execution\n",
    "\n",
    "Here, you're testing the actual execution of your `extract_video_id` tool with a real YouTube URL. You can call the tool using the `.run()` method, which is a convenient way to execute the tool directly and see its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40cd9176-a895-49c8-af36-95721c12010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hfIUstzHs9A'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id.run(\"https://www.youtube.com/watch?v=hfIUstzHs9A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a5b29ba-048b-4d1e-a1c1-ae26617db860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='extract_video_id', description='Extracts the 11-character YouTube video ID from a URL.\\n\\nArgs:\\n    url (str): A YouTube URL containing a video ID.\\n\\nReturns:\\n    str: Extracted video ID or error message if parsing fails.', args_schema=<class 'langchain_core.utils.pydantic.extract_video_id'>, func=<function extract_video_id at 0x1352985e0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ce3f4-e6d0-4347-9686-78ce42746f2e",
   "metadata": {},
   "source": [
    "This output shows that your function has been transformed into a `StructuredTool` object by LangChain. It displays the tool's name ('extract_video_id'), its description (our docstring), a Pydantic schema for input validation, and a reference to your original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d0dfc-38e7-4edc-ac5d-f18a880aa376",
   "metadata": {},
   "source": [
    "## Tool list \n",
    "Multiple tools will be created to enhance the LLM's capabilities. For organization, create a list called tools, which is a standard Python list that contains tool objects created with the @tool decorator. This list doesn't execute functions or determine call order - it simply collects tool objects in one place so they can be efficiently passed to the language model via llm.bind_tools(tools). This approach allows the LLM to access all available tools without requiring them to be individually registered.\n",
    "\n",
    "Adding the ```extract_video_id``` tool to your tools list, which you can later provide to the LLM so it can use this functionality when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4408de-db43-4edd-978b-ba530087e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tools.append(extract_video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e4a38-e1b5-4cf0-9029-230e1413f7f1",
   "metadata": {},
   "source": [
    "Now that you have understood the basic structure, let's define the rest of the tools you'll need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4af6c8-1b01-4945-86d3-06b07a3908b3",
   "metadata": {},
   "source": [
    "### Defining transcript fetching tool\n",
    "\n",
    "Now you're going to create another tool that fetches the transcript from a YouTube video. This tool uses the `YouTubeTranscriptApi` library to retrieve the captions or subtitles from a video. You'll be taking the video ID (which can be extracted using your previous tool) and an optional language parameter. The function attempts to get the transcript and joins all text segments into a continuous string, or returns an error message if the transcript can't be retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487ccc76-11b4-4f33-b47d-85acd5906bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_transcript(video_id: str, language: str = \"en\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video.\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n",
    "        language (str): Language code for the transcript (e.g., \"en\", \"es\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcript text or an error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ytt_api = YouTubeTranscriptApi()\n",
    "        transcript = ytt_api.fetch(video_id, languages=[language])\n",
    "        return \" \".join([snippet.text for snippet in transcript.snippets])\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff94ef-3de2-4cdd-9320-dee31ade8499",
   "metadata": {},
   "source": [
    "Let's test the fetch_transcript tool by directly calling it with the .run() method on a specific video ID. This will attempt to retrieve the transcript for the video with ID \"hfIUstzHs9A\" in the default English language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f87bf1-581f-4997-8eda-30829f444e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the past couple of months, large language models, or LLMs, such as chatGPT, have taken the world by storm. Whether it\\'s writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value. My name is Kate Soule. I\\'m a senior manager of business strategy at IBM Research, and today I\\'m going to give a brief overview of this new field of AI that\\'s emerging and how it can be used in a business setting to drive value. Now, large language models are actually a part of a different class of models called foundation models. Now, the term \"foundation models\" was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm. Where before AI applications were being built by training, maybe a library of different AI models, where each AI model was trained on very task-specific data to perform very specific task. They predicted that we were going to start moving to a new paradigm, where we would have a foundational capability, or a foundation model, that would drive all of these same use cases and applications. So the same exact applications that we were envisioning before with conventional AI, and the same model could drive any number of additional applications. The point is that this model could be transferred to any number of tasks. What gives this model the super power to be able to transfer to multiple different tasks and perform multiple different functions is that it\\'s been trained on a huge amount, in an unsupervised manner, on unstructured data. And what that means, in the language domain, is basically I\\'ll feed a bunch of sentences-- and I\\'m talking terabytes of data here --to train this model. And the start of my sentence might be \"no use crying over spilled\" and the end of my sentence might be \"milk\". And I\\'m trying to get my model to predict the last word of the sentence based off of the words that it saw before. And it\\'s this generative capability of the model-- predicting and generating the next word --based off of previous words that it\\'s seen beforehand, that is why that foundation models are actually a part of the field of AI called generative AI because we\\'re generating something new in this case, the next word in a sentence. And even though these models are trained to perform, at its core, a generation past, predicting the next word in the sentence, we actually can take these models, and if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks-- things like classification, or named-entity recognition --things that you don\\'t normally associate as being a generative-based model or capability. And this process is called tuning. Where you can tune your foundation model by introducing a small amount of data, you update the parameters of your model and now perform a very specific natural language task. If you don\\'t have data, or have only very few data points, you can still take these foundation models and they actually work very well in low-labeled data domains. And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks. So an example of prompting a model to perform a classification task might be you could give a model a sentence and then ask it a question: Does this sentence have a positive sentiment or negative sentiment? The model\\'s going to try and finish generating words in that sentence, and the next natural word in that sentence would be the answer to your classification problem, which would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be. And these models work surprisingly well when applied to these new settings and domains. Now, this is a lot of where the advantages of foundation models come into play. So if we talk about the advantages, the chief advantage is the performance. These models have seen so much data. Again, data with a capital D-- terabytes of data --that by the time that they\\'re applied to small tasks, they can drastically outperform a model that was only trained on just a few data points. The second advantage of these models are the productivity gains. So just like I said earlier, through prompting or tuning, you need far less label data to get to task-specific model than if you had to start from scratch because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task. With these advantages, there are also some disadvantages that are important to keep in mind. And the first of those is the compute cost. So that penalty for having this model see so much data is that they\\'re very expensive to train, making it difficult for smaller enterprises to train a foundation model on their own. They\\'re also expensive-- by the time they get to a huge size, a couple billion parameters --they\\'re also very expensive to run inference. You might require multiple GPUs at a time just to host these models and run inference, making them a more costly method than traditional approaches. The second disadvantage of these models is on the trustworthiness side. So just like data is a huge advantage for these models, they\\'ve seen so much unstructured data, it also comes at a cost, especially in the domain like language. A lot of these models are trained basically off of language data that\\'s been scraped from the Internet. And there\\'s so much data that these models have been trained on. Even if you had a whole team of human annotators, you wouldn\\'t be able to go through and actually vet every single data point to make sure that it wasn\\'t biased and didn\\'t contain hate speech or other toxic information. And that\\'s just assuming you actually know what the data is. Often we don\\'t even know-- for a lot of these open source models that have been posted --what the exact datasets are that these models have been trained on leading to trustworthiness issues. So IBM recognizes the huge potential of these technologies. But my partners in IBM Research are working on multiple different innovations to try and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting. All of these examples that I\\'ve talked through so far have just been on the language side. But the reality is, there are a lot of other domains that foundation models can be applied towards. Famously, we\\'ve seen foundation models for vision --looking at models such as DALL-E 2, which takes text data, and that\\'s then used to generate a custom image. We\\'ve seen models for code with products like Copilot that can help complete code as it\\'s being authored. And IBM\\'s innovating across all of these domains. So whether it\\'s language models that we\\'re building into products like Watson Assistant and Watson Discovery, vision models that we\\'re building into products like Maximo Visual Inspection, or Ansible code models that we\\'re building with our partners at Red Hat under Project Wisdom. We\\'re innovating across all of these domains and more. We\\'re working on chemistry. So, for example, we just published and released molformer, which is a foundation model to promote molecule discovery or different targeted therapeutics. And we\\'re working on models for climate change, building Earth Science Foundation models using geospatial data to improve climate research. I hope you found this video both informative and helpful. If you\\'re interested in learning more, particularly how IBM is working to improve some of these disadvantages, making foundation models more trustworthy and more efficient, please take a look at the links below. Thank you.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript.run(\"hfIUstzHs9A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ae24d-85e2-474f-b52b-545713ed307d",
   "metadata": {},
   "source": [
    "---\n",
    "Adding the `fetch_transcript` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0900e2-afa3-45dd-8811-056d0ccce052",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(fetch_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5597b-0cfa-4931-97a8-4d49d50751f1",
   "metadata": {},
   "source": [
    "### Defining YouTube search tool\n",
    "\n",
    "Now let's create a search tool that allows finding videos on YouTube based on a query string. This tool uses the `Search` class from the PyTube library to perform searches on YouTube. When given a search term, it returns a list of matching videos with each video represented as a dictionary containing the title, video ID, and a shortened URL. This tool will be helpful for discovering relevant videos when you don't already have a specific URL in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad20d8ab-c154-44f4-afc4-289599223710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import Search\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def search_youtube(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search YouTube for videos matching the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search term to look for on YouTube\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing video titles and IDs in format:\n",
    "        [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n",
    "        Returns error message if search fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Search(query)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": yt.title,\n",
    "                \"video_id\": yt.video_id,\n",
    "                \"url\": f\"https://youtu.be/{yt.video_id}\"\n",
    "            }\n",
    "            for yt in s.results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bce386-6518-44a0-b3b9-39f6ecbe4601",
   "metadata": {},
   "source": [
    "Now, you'll test your `search_youtube` tool by calling it with the `.run()` method and the search query \"Generative AI.\" This will return a list of YouTube videos related to generative AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd64a076-1933-4d5c-a9e7-d4895eb09a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_out=search_youtube.run(\"Generative AI\")\n",
    "display(JSON(search_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c7aa0-4b07-4ef2-965e-7676d8085ded",
   "metadata": {},
   "source": [
    "Appending the `search_youtube` tool to tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a89df6df-8a86-422b-bdc6-c205ea9ee360",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(search_youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f09467-b84c-47a9-b244-1c9a4f01794f",
   "metadata": {},
   "source": [
    "### Defining metadata extraction tool\n",
    "\n",
    "Now you'll create a tool that extracts detailed metadata from a YouTube video using the `yt-dlp` library. This tool takes a YouTube URL and returns comprehensive information about the video, including its title, view count, duration, channel name, like count, comment count, and any chapter markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3190da0f-2152-4254-9db6-edda32c7859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_full_metadata(url: str) -> dict:\n",
    "    \"\"\"Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.\"\"\"\n",
    "    with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        return {\n",
    "            'title': info.get('title'),\n",
    "            'views': info.get('view_count'),\n",
    "            'duration': info.get('duration'),\n",
    "            'channel': info.get('uploader'),\n",
    "            'likes': info.get('like_count'),\n",
    "            'comments': info.get('comment_count'),\n",
    "            'chapters': info.get('chapters', [])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402e3b8-83b0-456a-91e7-ef005b0f1e80",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_full_metadata` tool by running it on a specific YouTube video URL. This will extract comprehensive information about the video with ID \"qWHaMrR5WHQ\" without downloading the actual video content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e0f59ce-fe9d-4534-9ee6-778755a2c8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "channel": "LetsLearnWithChinnoVino",
       "chapters": null,
       "comments": 5,
       "duration": 1048,
       "likes": 23,
       "title": "Explained how to use Tools (tool calling) in LangChain",
       "views": 1415
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_data=get_full_metadata.run(\"https://youtu.be/qWHaMrR5WHQ\")\n",
    "display(JSON(meta_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e192490-7f45-4cd9-a6cf-acad237d3aa4",
   "metadata": {},
   "source": [
    "Adding the `get_full_metadata` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6569c684-e6a9-425e-9c87-ad864dbd4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_full_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446efb2-0508-4328-a68f-f595c6c92742",
   "metadata": {},
   "source": [
    "### Defining trending videos tool\n",
    "\n",
    "Now, you'll create a tool to fetch the currently trending videos on YouTube for a specific region. This tool uses `yt-dlp` to access YouTube's trending feed based on a provided country code (like \"US\" for the United States or \"IN\" for India). It collects important information about each trending video, including the title, video ID, URL, channel name, duration, and view count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "55cc6020-482b-408b-9733-c268cf52a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_trending_videos(region_code: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches currently trending YouTube videos for a specific region.\n",
    "    \n",
    "    Args:\n",
    "        region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with video details: title, video_id, channel, view_count, duration\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        'geo_bypass_country': region_code.upper(),\n",
    "        'extract_flat': True,\n",
    "        'quiet': True,\n",
    "        'force_generic_extractor': True,\n",
    "        'logger': yt_dpl_logger\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(\n",
    "                f'https://www.youtube.com/feed/trending',   # not working https://www.youtube.com/feed/trending\n",
    "                # 'https://www.youtube.com/charts',\n",
    "                download=False\n",
    "            )\n",
    "            print(info)\n",
    "            \n",
    "            trending_videos = []\n",
    "            for entry in info['entries']:\n",
    "                video_data = {\n",
    "                    'title': entry.get('title', 'N/A'),\n",
    "                    'video_id': entry.get('id', 'N/A'),\n",
    "                    'url': entry.get('url', 'N/A'),\n",
    "                    'channel': entry.get('uploader', 'N/A'),\n",
    "                    'duration': entry.get('duration', 0),\n",
    "                    'view_count': entry.get('view_count', 0)\n",
    "                }\n",
    "                trending_videos.append(video_data)\n",
    "                \n",
    "            return trending_videos[:25]  # Return top 25 trending videos\n",
    "            \n",
    "    except Exception as e:\n",
    "        return [{'error': f\"Failed to fetch trending videos: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536ba94-1fbd-4a78-b4a5-da47b0689e95",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_trending_videos` tool by running it with the region code `\"US\"` to fetch trending videos from the United States. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13fb723e-29bc-4034-a55a-3151e7268c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n"
     ]
    },
    {
     "data": {
      "application/json": "",
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trending_videos=get_trending_videos.run(\"US\")\n",
    "# Display as formatted JSON\n",
    "display(JSON(trending_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'error': 'Failed to fetch trending videos: ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page'}]\n"
     ]
    }
   ],
   "source": [
    "print(trending_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ace04d-5f93-49a4-bf0b-9839a740973f",
   "metadata": {},
   "source": [
    "Now, let's add the `get_trending_videos` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f56b7258-ce43-462d-9746-2cc3ba56c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools.append(get_trending_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475a4e6-7224-4022-aec2-a651d222e364",
   "metadata": {},
   "source": [
    "### Defining thumbnail retrieval tool\n",
    "\n",
    "Now you'll create a tool to extract all available thumbnail images for a YouTube video. This tool uses `yt-dlp` to retrieve information about the various thumbnail images that YouTube generates for videos at different resolutions. For each thumbnail, collect its URL, width, height, and formatted resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1dbcb768-0de9-4f69-a7a5-ca7ede6d8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_thumbnails(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get available thumbnails for a YouTube video using its URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): YouTube video URL (any format)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with thumbnail URLs and resolutions in YouTube's native order\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            \n",
    "            thumbnails = []\n",
    "            for t in info.get('thumbnails', []):\n",
    "                if 'url' in t:\n",
    "                    thumbnails.append({\n",
    "                        \"url\": t['url'],\n",
    "                        \"width\": t.get('width'),\n",
    "                        \"height\": t.get('height'),\n",
    "                        \"resolution\": f\"{t.get('width', '')}x{t.get('height', '')}\".strip('x')\n",
    "                    })\n",
    "            \n",
    "            return thumbnails\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Failed to get thumbnails: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3f145-cdc7-4682-9ddf-80ec57501e0c",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_thumbnails` tool by running it on a specific YouTube video URL. This will extract information about all available thumbnail images for the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ea66ac2-6aa0-4c07-b58c-f77c82a96287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thumbnails=get_thumbnails.run(\"https://www.youtube.com/watch?v=qWHaMrR5WHQ\")\n",
    "\n",
    "display(JSON(thumbnails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/3.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/3.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/2.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/2.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/1.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/1.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/mq3.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq3.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/mq2.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq2.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/mq1.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq1.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hq3.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq3.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hq2.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq2.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hq1.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq1.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/sd3.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd3.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/sd2.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd2.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/sd1.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd1.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/default.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/default.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/mqdefault.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mqdefault.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/0.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/0.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDK33nNmNAuVa1ibN7uTkq5okdwfw',\n",
       "  'width': 168,\n",
       "  'height': 94,\n",
       "  'resolution': '168x94'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEiCKgBEF5IWvKriqkDFQgBFQAAAAAYASUAAMhCPQCAokN4AQ==&rs=AOn4CLBQiFoh8Nia1_XrFTBE3-g4hp61xQ',\n",
       "  'width': 168,\n",
       "  'height': 94,\n",
       "  'resolution': '168x94'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCMQBEG5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBa9uAOnYdtfae6yOMFyYfUFEvjkA',\n",
       "  'width': 196,\n",
       "  'height': 110,\n",
       "  'resolution': '196x110'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEiCMQBEG5IWvKriqkDFQgBFQAAAAAYASUAAMhCPQCAokN4AQ==&rs=AOn4CLDw2g-OpL84ZvPbhVdQ9S_Unp3RMg',\n",
       "  'width': 196,\n",
       "  'height': 110,\n",
       "  'resolution': '196x110'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCPYBEIoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLDjZI5FIYKE2PlZhm9PQTk0eKeDAw',\n",
       "  'width': 246,\n",
       "  'height': 138,\n",
       "  'resolution': '246x138'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEjCPYBEIoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLCJk1-Lqfb8XHsFyiBDiCN0opMcDA',\n",
       "  'width': 246,\n",
       "  'height': 138,\n",
       "  'resolution': '246x138'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLBTNOqMuqjK5oJ-_mtGsUJOh-sVkA',\n",
       "  'width': 336,\n",
       "  'height': 188,\n",
       "  'resolution': '336x188'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEjCNACELwBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAlaeOG0AijM70t7qsBkkaEvRb3FQ',\n",
       "  'width': 336,\n",
       "  'height': 188,\n",
       "  'resolution': '336x188'},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg',\n",
       "  'width': 480,\n",
       "  'height': 360,\n",
       "  'resolution': '480x360'},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hqdefault.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/sddefault.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sddefault.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/hq720.jpg',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq720.webp',\n",
       "  'width': None,\n",
       "  'height': None,\n",
       "  'resolution': ''},\n",
       " {'url': 'https://i.ytimg.com/vi/qWHaMrR5WHQ/maxresdefault.jpg',\n",
       "  'width': 1920,\n",
       "  'height': 1080,\n",
       "  'resolution': '1920x1080'},\n",
       " {'url': 'https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/maxresdefault.webp',\n",
       "  'width': 1920,\n",
       "  'height': 1080,\n",
       "  'resolution': '1920x1080'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thumbnails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eb98a-0b59-4f44-a25d-faa4b95fb1b2",
   "metadata": {},
   "source": [
    "Now, let's add the `get_thumbnails` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "643cc50c-e671-4f8e-9aac-54590a8dfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_thumbnails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505eead-4c7e-42a3-83e0-7b12a7caccbe",
   "metadata": {},
   "source": [
    "##  Binding tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b06234-31ac-41b9-bb43-c091b842f5a7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, you'll bind your collection of tools to the language model. It enables the LLM to access and use your custom YouTube tools during conversations. By binding the tools, you're giving the model the ability to call these functions when it determines they're needed to fulfill a user request, making the LLM aware of your tools' capabilities and how to use them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "540106a6-da72-4cda-b6bf-08cccbbeb5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21719018-2494-4057-bcf7-c966d0932373",
   "metadata": {},
   "source": [
    "The ```bind_tools()``` function passes all this information to the language model. It converts each tool's attributes (name, description, parameters schema) into a standardized format that the LLM can understand and use to determine when and how to call specific tools based on user requests. Similar to the following code where the schema for each tool is stored:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52284139-af0f-40bf-a425-cb8671129622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
       "name": "extract_video_id",
       "parameters": {
        "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "extract_video_id",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
       "name": "fetch_transcript",
       "parameters": {
        "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
        "properties": {
         "language": {
          "default": "en",
          "title": "Language",
          "type": "string"
         },
         "video_id": {
          "title": "Video Id",
          "type": "string"
         }
        },
        "required": [
         "video_id"
        ],
        "title": "fetch_transcript",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Search YouTube for videos matching the query.\n\nArgs:\n    query (str): The search term to look for on YouTube\n\nReturns:\n    List of dictionaries containing video titles and IDs in format:\n    [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n    Returns error message if search fails",
       "name": "search_youtube",
       "parameters": {
        "description": "Search YouTube for videos matching the query.\n\nArgs:\n    query (str): The search term to look for on YouTube\n\nReturns:\n    List of dictionaries containing video titles and IDs in format:\n    [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n    Returns error message if search fails",
        "properties": {
         "query": {
          "title": "Query",
          "type": "string"
         }
        },
        "required": [
         "query"
        ],
        "title": "search_youtube",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
       "name": "get_full_metadata",
       "parameters": {
        "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_full_metadata",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
       "name": "get_thumbnails",
       "parameters": {
        "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_thumbnails",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    schema = {\n",
    "   \"name\": tool.name,\n",
    "   \"description\": tool.description,\n",
    "   \"parameters\": tool.args_schema.schema() if tool.args_schema else {},\n",
    "   \"return\": tool.return_type if hasattr(tool, \"return_type\") else None}\n",
    "    display(JSON(schema))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15b37e-f5b9-4d8d-96cf-045d4ac914e3",
   "metadata": {},
   "source": [
    "### How the LLM calls a tool\n",
    "\n",
    "Now, define a sample user query that asks for a summary of a specific YouTube video. This query will be used to demonstrate how your LLM can understand a natural language request and use the appropriate tools you've provided to fulfill it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b33ec25b-4c80-4d24-94ea-74cac124fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d75e1-9dd3-450e-a172-f1b28e5d0f3f",
   "metadata": {},
   "source": [
    "Repeating a message object to represent your user query. You'll be wrapping the query string in a HumanMessage object, which is the standard way to format user inputs in LangChain. It represents a human message as a person is expected to initiate the interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86597539-e056-4a22-95bf-48f9e61f51e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content = query)]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d6847-d9d4-4e56-b073-e31cf221e4db",
   "metadata": {},
   "source": [
    "### LangChain tool binding process\n",
    "\n",
    "This step involves sending your message to the LLM and storing its response. Here you'll invoke the language model with your user query about summarizing a YouTube video. The response will contain both text content and potentially tool calls that the model decides to make. ``response_1`` contains the LLM's response to the user message, including any tool calls it decides to make. The response object contains the content of the LLM's reply plus structured information about which tools it wants to call and with what parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e918ce88-d41d-46e8-a008-a3efbff27b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user wants to summarize a YouTube video. The URL provided is https://www.youtube.com/watch?v=T-D1OfcDW1M. Let me figure out how to approach this.\\n\\nFirst, I need to extract the video ID from the URL. The function extract_video_id is designed for that. The URL has the parameter 'v' followed by the video ID, which in this case is T-D1OfcDW1M. So I'll call extract_video_id with the URL to confirm the video ID.\\n\\nOnce I have the video ID, the next step is to get the transcript of the video. The fetch_transcript function requires the video_id and a language code. The user specified English, so the default 'en' should work. Using the video ID from the previous step, I'll fetch the transcript.\\n\\nWith the transcript in hand, I can then summarize it. However, looking at the available tools, there's no direct function for summarizing text. The user might expect me to use the transcript to create a summary manually. But since the tools provided don't include a summarization function, I need to rely on the transcript retrieval first.\\n\\nWait, maybe I should check if there's a function I missed. The available functions are extract_video_id, fetch_transcript, search_youtube, get_full_metadata, and get_thumbnails. No summarization tool. So after fetching the transcript, I'll have to process it myself to create a summary.\\n\\nSo the steps are: extract video ID, fetch transcript, then summarize the transcript text. Since the user wants the summary in English, which aligns with the transcript language. Let me proceed step by step.\\n\", 'tool_calls': [{'id': 'xs8v0z744', 'function': {'arguments': '{\"url\":\"https://www.youtube.com/watch?v=T-D1OfcDW1M\"}', 'name': 'extract_video_id'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 380, 'prompt_tokens': 704, 'total_tokens': 1084, 'completion_time': 0.733666255, 'prompt_time': 0.031519503, 'queue_time': 0.104157925, 'total_time': 0.765185758}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4f87baee-d7fb-4c36-8a0a-fba704cd08dc-0', tool_calls=[{'name': 'extract_video_id', 'args': {'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}, 'id': 'xs8v0z744', 'type': 'tool_call'}], usage_metadata={'input_tokens': 704, 'output_tokens': 380, 'total_tokens': 1084})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(messages)\n",
    "response_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fdf74-67ed-4ea6-a0bf-6ed8684430d7",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58fc629d-d1b8-4d64-b548-e3d2fd15bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2eadbf-6303-4876-9c75-287719199e91",
   "metadata": {},
   "source": [
    "### Extracting tool call information\n",
    "After receiving the LLM's response, you need to extract the structured tool call information. The line tool_calls_1 = response_1.tool_calls gets the tool call objects that contain which tool the LLM has decided to use and what parameters to pass to it. This information will be used to execute the appropriate tool with the correct inputs.\n",
    "\n",
    "#### Creating a tool mapping dictionary\n",
    "\n",
    "Now you'll create a dictionary that maps tool names to their corresponding function objects. This mapping will be useful later when you need to programmatically invoke specific tools based on their names. It allows you to easily look up and execute a tool function when you have only the tool name as a string, which will be important when processing tool calls from the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc23bf71-08cd-4d10-a236-77deabb62ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"get_thumbnails\" : get_thumbnails,\n",
    "    \"get_trending_videos\": get_trending_videos,\n",
    "    \"extract_video_id\": extract_video_id,\n",
    "    \"fetch_transcript\": fetch_transcript,\n",
    "    \"search_youtube\": search_youtube,\n",
    "    \"get_full_metadata\": get_full_metadata\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245ead2-f9cc-4b04-a718-76a613ed3769",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's response. When the LLM determines it needs to use one of your tools, it includes structured \"tool_calls\" in its response. Here, you're accessing those tool calls to see which tools the model decided to use in order to fulfill the request about summarizing the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "678a66ff-785d-43d4-a67e-264fdaab9ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'extract_video_id',\n",
       "  'args': {'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'},\n",
       "  'id': 'xs8v0z744',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "display(tool_calls_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35425124-dfb9-4094-928a-a0318ece51c2",
   "metadata": {},
   "source": [
    "Here you're seeing the structure of the tool call that the LLM decided to make. The tool call is formatted as a dictionary with the following key components:\n",
    "\n",
    "1. `name`: 'extract_video_id' - This identifies which tool the LLM wants to use first (the video ID extraction tool)\n",
    "2. `args`: Contains the arguments to pass to the tool - in this case, the YouTube URL from your query\n",
    "3. `id`: A unique identifier for this specific tool call, which helps track the request/response pair\n",
    "4. `type`: Indicates this is a tool call rather than other types of AI responses\n",
    "\n",
    "This shows that the LLM correctly understood it needs to first extract the video ID from the URL before it can proceed with summarizing the video content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c0f1c-b920-493a-b289-b54863c1753f",
   "metadata": {},
   "source": [
    "Accessing the name of the first tool that the LLM decided to use. Here you're extracting just the name component `('extract_video_id')` from the first tool call in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81278d07-190a-478a-82aa-d360dc0a14ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n"
     ]
    }
   ],
   "source": [
    "tool_name=tool_calls_1[0]['name']\n",
    "print(tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cbb0f-94b2-4172-ab7e-eb391b0acb28",
   "metadata": {},
   "source": [
    "You need a tool ID to help the LLM know where the output came from:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9dd1cd9d-7c01-47ee-b290-bab0e27f3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs8v0z744\n"
     ]
    }
   ],
   "source": [
    "tool_call_id =tool_calls_1[0]['id']\n",
    "print(tool_call_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dac21-4e79-49b7-97ac-d68fda5ee3cf",
   "metadata": {},
   "source": [
    "Accessing the arguments that need to be passed to the chosen tool. Here, you're extracting the arguments component from the first tool call, which contains the YouTube URL that needs to be processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef146063-6c3d-4897-82b5-df9be7a29d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}\n"
     ]
    }
   ],
   "source": [
    "args=tool_calls_1[0]['args']\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983b2ac-d5fd-4db0-935e-62fca70368c5",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), you append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f7a29-b166-4842-be26-503d651f3b7c",
   "metadata": {},
   "source": [
    "Executing the tool call that the LLM requested. Here, you're using your tool mapping dictionary to:\n",
    "1. Look up the appropriate function based on the tool name ('extract_video_id')\n",
    "2. Call that function with the arguments provided by the LLM\n",
    "3. Capture the output (the extracted video ID)\n",
    "\n",
    "This shows how you can programmatically execute the tools that the LLM decided to use. First, you get the tool from ```tool_mapping```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b39ca2c-3cf9-4469-b466-64e904c1bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tool=tool_mapping[tool_calls_1[0]['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b1d03-dee7-4f20-a9b9-1106bb18e2fc",
   "metadata": {},
   "source": [
    "You'll then call the tool with the arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "324235b0-1f95-40ca-80ac-8da59e5737dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-D1OfcDW1M'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id =my_tool.invoke(tool_calls_1[0]['args'])\n",
    "video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd92cd-ace4-4ceb-9f30-566cfc96a347",
   "metadata": {},
   "source": [
    "Adding the tool's output to your conversation history. You'll create a `ToolMessage` that contains:\n",
    "1. The result from executing the tool (the extracted video ID)\n",
    "2. The original tool call ID to link this response back to the specific request\n",
    "\n",
    "By appending this message to your conversation history, you're informing the LLM about the results of the tool execution, which it can use in its next response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31e145a9-8dcf-4923-8876-26252585dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = video_id, tool_call_id = tool_calls_1[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca62ec-f90a-4e61-9c67-35ef26d5c562",
   "metadata": {},
   "source": [
    "Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2a4b4fe-1e48-4d6e-adae-b953c84f7a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user wants to summarize a YouTube video. They provided the URL, so first I need to get the video ID. I already used the extract_video_id function and got the ID T-D1OfcDW1M. Next step is to fetch the transcript using that video ID. The fetch_transcript function requires the video_id and a language, which the user specified as English. So I'll call fetch_transcript with those parameters. Once I have the transcript, I can start summarizing it. I should make sure the transcript is successfully retrieved before proceeding. If there's an error, I'll need to inform the user. But assuming it works, I'll process the text to create a concise summary.\\n\", 'tool_calls': [{'id': '9nrhpbnxs', 'function': {'arguments': '{\"language\":\"en\",\"video_id\":\"T-D1OfcDW1M\"}', 'name': 'fetch_transcript'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 184, 'prompt_tokens': 760, 'total_tokens': 944, 'completion_time': 0.318976268, 'prompt_time': 0.034529989, 'queue_time': 0.125229041, 'total_time': 0.353506257}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8527cf4a-c9ca-43a7-95bb-136607d84789-0', tool_calls=[{'name': 'fetch_transcript', 'args': {'language': 'en', 'video_id': 'T-D1OfcDW1M'}, 'id': '9nrhpbnxs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 760, 'output_tokens': 184, 'total_tokens': 944})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = llm_with_tools.invoke(messages)\n",
    "response_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa5d8c-9b14-40b6-b9ba-6d9a6923cf1e",
   "metadata": {},
   "source": [
    "The result is a AI messege! Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, you'll invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e59ec775-e911-4064-a3b8-fe3d1b4a182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7602b7-b0e5-4023-8f9d-0dc606b20720",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's second response. After receiving the video ID, the LLM will likely decide to use another tool to help with the summarization task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c09e920c-a502-4fa7-b65f-e636d0511852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fetch_transcript',\n",
       "  'args': {'language': 'en', 'video_id': 'T-D1OfcDW1M'},\n",
       "  'id': '9nrhpbnxs',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_2 = response_2.tool_calls\n",
    "tool_calls_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473119eb-d666-4442-8208-af30d3a15c71",
   "metadata": {},
   "source": [
    "Here, you can see that the LLM has decided to use the `fetch_transcript` tool as its next step. \n",
    "\n",
    "The model is passing two arguments to the transcript fetching tool:\n",
    "1. `video_id`: 'T-D1OfcDW1M' - The ID that was extracted from the original YouTube URL\n",
    "2. `language`: 'en' - Requesting the transcript in English as specified in the user's query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a18237-6910-43b8-9542-2351c2cedff1",
   "metadata": {},
   "source": [
    "---\n",
    "Fetching the transcript using the video ID obtained in the previous step. Here, you're executing the second tool that the LLM requested by:\n",
    "1. Looking up the appropriate function `('fetch_transcript')` from your tool mapping\n",
    "2. Invoking it with the video ID and language parameters\n",
    "3. Storing the resulting transcript content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a34da08-2b0c-4eaa-ac05-013ab277b9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name\\xa0is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want\\xa0to tell you about a framework to help large language models be more accurate and more up to\\xa0date: Retrieval-Augmented Generation, or RAG. Let\\'s just talk about the \"Generation\" part for a\\xa0minute. So forget the \"Retrieval-Augmented\". So the\\xa0generation, this refers to large language models,\\xa0or LLMs, that generate text in response to a user query, referred to as a prompt. These\\xa0models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question: \"In our solar system, what planet has the most\\xa0moons?\" And my response was, â€œOh, that\\'s really great that you\\'re asking this question. I loved\\xa0space when I was your age.â€ Of course, that was like 30 years ago. But I know this! I read an\\xa0article and the article said that it was Jupiter and 88 moons. So that\\'s the answer. Now, actually,\\xa0there\\'s a couple of things wrong with my answer. First of all, I have no source to support what\\xa0I\\'m saying. So even though I confidently said â€œI read an article, I know the answer!â€, I\\'m not\\xa0sourcing it. I\\'m giving the answer off the top of my head. And also, I actually haven\\'t kept up with\\xa0this for awhile, and my answer is out of date. So we have two problems here. One is no source.\\xa0And the second problem is that I am out of date.\\xa0\\xa0 And these, in fact, are two behaviors that are\\xa0often observed as problematic when interacting with large language models. Theyâ€™re LLM\\xa0challenges. Now, what would have happened if I\\'d taken a beat and first gone and looked\\xa0up the answer on a reputable source like NASA? Well, then I would have been able to say, â€œAh,\\xa0okay! So the answer is Saturn with 146 moons.â€ And in fact, this keeps changing because scientists\\xa0keep on discovering more and more moons. So I have now grounded my answer in something more\\xa0\\nbelievable. I have not hallucinated or made up an answer. Oh, by the way, I didn\\'t leak personal\\xa0information about how long ago it\\'s been since I was obsessed with space. All right, so what does\\xa0this have to do with large language models? Well, how would a large language model have answered\\xa0this question? So let\\'s say that I have a user asking this question about moons. A large language\\xa0model would confidently say, OK, I have been trained and from what I know in my parameters\\xa0during my training, the answer is Jupiter. The answer is wrong. But, you know, we don\\'t know. The large language model is very confident in what it answered. Now, what happens when you add this\\xa0retrieval augmented part here? What does that mean? That means that now, instead of just relying\\xa0on what the LLM knows, we are adding a content store. This could be open like the internet. This\\xa0can be closed like some collection of documents, collection of policies, whatever. The point,\\xa0though, now is that the LLM first goes and talks to the content store and says,\\xa0â€œHey, can you retrieve for me information that is relevant to what the user\\'s\\xa0query was?â€ And now, with this retrieval-augmented answer, it\\'s not Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, â€œOh, okay, I know\\xa0the response. Here it is. Here\\'s my response.â€\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and retrieve the most up to date information. The second problem, source. Well, the large language model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, â€œI don\\'t know.â€ If\\xa0the user\\'s question cannot be reliably answered based on your data store, the model should say,\\xa0\"I don\\'t know,\" instead of making up something that is believable and may mislead the user. This\\xa0can have a negative effect as well though, because if the retriever is not sufficiently\\xa0good to give the large language model the best, most high-quality grounding information, then\\xa0maybe the user\\'s query that is answerable doesn\\'t get an answer. So this is actually why lots\\xa0of folks, including many of us here at IBM, are working the problem on both sides. We are both\\xa0working to improve the retriever to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript_tool_output = tool_mapping[tool_calls_2[0]['name']].invoke(tool_calls_2[0]['args'])\n",
    "fetch_transcript_tool_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d3cd1-1cdf-4d53-bca5-36d5e59abd79",
   "metadata": {},
   "source": [
    "---\n",
    "You're adding the transcript content to your conversation history by creating another `ToolMessage` that contains the transcript text and the ID of the tool call that requested it. This gives the LLM access to the actual video content so it can generate a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "88f1f10c-9049-4c14-b27f-15d8a4a23e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = fetch_transcript_tool_output, tool_call_id = tool_calls_2[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07513f38-a83b-417e-9f26-67c9138bf04b",
   "metadata": {},
   "source": [
    "Generating the final summary by sending your complete conversation history to the LLM. Now that the model has access to both the video ID and the full transcript, you'll invoke it one more time to generate the summary that the user requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "46470f3c-9195-43f9-a92e-a00d8a8bf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "368bea39-a4d3-4e8f-b235-b4265f1e756a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Summary of the YouTube Video on Retrieval-Augmented Generation (RAG):**\\n\\nThe video explains how **Retrieval-Augmented Generation (RAG)** enhances the accuracy and reliability of large language models (LLMs). The speaker, Marina Danilevsky, uses an example about planetary moons to illustrate LLM limitations: models might provide outdated or incorrect answers (e.g., claiming Jupiter has 88 moons instead of Saturnâ€™s 146) and lack citations. \\n\\n**RAG addresses this by:**  \\n1. **Adding a retrieval step**: The model consults a content store (e.g., databases, the internet) to fetch up-to-date, evidence-based information before generating an answer.  \\n2. **Combining retrieval with generation**: The LLM uses retrieved data alongside the userâ€™s query to produce a grounded response, reducing hallucination and improving accuracy.  \\n3. **Enabling \"I don\\'t know\" responses**: If the data store lacks reliable information, the model avoids making up answers.  \\n\\n**Challenges**: The effectiveness of RAG depends on the quality of the retriever (e.g., finding relevant, high-quality sources) and the generative modelâ€™s ability to synthesize information. Ongoing research aims to refine both components for better performance.  \\n\\nIn essence, RAG bridges the gap between LLMsâ€™ knowledge and real-time data, making them more trustworthy for critical applications.', additional_kwargs={'reasoning_content': \"Okay, let me break this down. The user wants a summary of the YouTube video in English. The video's transcript has been provided, which is about Retrieval-Augmented Generation (RAG) and how it helps large language models (LLMs) be more accurate and up-to-date.\\n\\nFirst, I need to understand the key points from the transcript. The speaker, Marina Danilevsky, uses an example about planets and moons to illustrate how RAG works. She explains that LLMs can be outdated and prone to hallucination because they rely solely on their training data. RAG addresses this by adding a retrieval step where the model consults a content store (like a database or the internet) before generating an answer. This makes the answers more accurate and evidence-based.\\n\\nThe main challenges with LLMs she mentions are being out of date and not citing sources. RAG solves these by allowing the model to access current information and provide references. She also mentions that if the retriever isn't good enough, the model might fail to answer answerable questions, which is why improving both retrieval and generation is important.\\n\\nNow, I need to condense this into a concise summary. Start by stating the main topic: RAG. Then mention the problem with LLMs, the solution (RAG), how it works, and the benefits. Include the example about Saturn and Jupiter to make it relatable. Also, note the challenges in implementing RAG effectively.\\n\\nMake sure the summary is in English and flows naturally, highlighting the key points without getting too technical. Avoid jargon where possible, but since RAG is a specific term, it's okay to use it. Check that all important aspects from the transcript are covered: accuracy, up-to-date info, citation, and the balance between retrieval and generation.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 660, 'prompt_tokens': 2190, 'total_tokens': 2850, 'completion_time': 1.503309442, 'prompt_time': 0.104699582, 'queue_time': 0.156467189, 'total_time': 1.608009024}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ef4c9316-d047-4c7d-849f-d0441304a89c-0', usage_metadata={'input_tokens': 2190, 'output_tokens': 660, 'total_tokens': 2850})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary of the YouTube Video on Retrieval-Augmented Generation (RAG):**\n",
      "\n",
      "The video explains how **Retrieval-Augmented Generation (RAG)** enhances the accuracy and reliability of large language models (LLMs). The speaker, Marina Danilevsky, uses an example about planetary moons to illustrate LLM limitations: models might provide outdated or incorrect answers (e.g., claiming Jupiter has 88 moons instead of Saturnâ€™s 146) and lack citations. \n",
      "\n",
      "**RAG addresses this by:**  \n",
      "1. **Adding a retrieval step**: The model consults a content store (e.g., databases, the internet) to fetch up-to-date, evidence-based information before generating an answer.  \n",
      "2. **Combining retrieval with generation**: The LLM uses retrieved data alongside the userâ€™s query to produce a grounded response, reducing hallucination and improving accuracy.  \n",
      "3. **Enabling \"I don't know\" responses**: If the data store lacks reliable information, the model avoids making up answers.  \n",
      "\n",
      "**Challenges**: The effectiveness of RAG depends on the quality of the retriever (e.g., finding relevant, high-quality sources) and the generative modelâ€™s ability to synthesize information. Ongoing research aims to refine both components for better performance.  \n",
      "\n",
      "In essence, RAG bridges the gap between LLMsâ€™ knowledge and real-time data, making them more trustworthy for critical applications.\n"
     ]
    }
   ],
   "source": [
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c914ee8-f163-407c-93a2-6aab161b9f9f",
   "metadata": {},
   "source": [
    "### Automating the tool calling process\n",
    "\n",
    "You manually saw how you input a text request to your LLM, where the LLM recognized that a tool call was required. Then, you extracted the tool content, formatted the input, made the next tool call, and repeated these steps. While this step-by-step approach helps understand the process, it would be tedious to implement for every application. Now let's automate this entire workflow.\n",
    "\n",
    "#### Extracting tool information from LLM response\n",
    "Create a function to automate tool calling. The input is the tool call object from which you extract the name, and use the tool_mapping dictionary to find the correct function to call. You'll pass the arguments from the tool call to this function and then send the output back as a ToolMessage with the tool_call_id included.\n",
    "The tool_call_id is an essential part of this process as it links each tool response back to the specific tool request made by the language model. This ID ensures the LLM can match responses to its requests, which is crucial when multiple tools are called in sequence or simultaneously. Without this ID, the LLM would have no way to know which response corresponds to which request, making multi-step reasoning impossible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6d56f61e-abc4-44f6-9cc4-4e3f8c70e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing steps\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        return ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ToolMessage(\n",
    "            content=f\"Error: {str(e)}\",\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71201a3-543f-4f3f-9ec6-d10157b6a886",
   "metadata": {},
   "source": [
    "You are now going to chain all your functions or tools together, but before you do so, you need to format the data properly. Not only are you required to store the output of each tool, but you also need to store state information like tool IDs. To do this effectively, you must ensure the output of each tool can be properly passed to the next step in your pipeline. The RunnablePassthrough component allows you to maintain state throughout the chain while adding or transforming data at each step, making it ideal for connecting your various tools into a cohesive workflow.\n",
    "The RunnableLambda, placed at the end of your chain, serves a different purpose - it extracts only the final result you want to present to the user. After all the tool calls and message processing, you have a rich state object with many fields, but the user typically only needs the final answer. The RunnableLambda transforms this complete state into just the information you want to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf2026ae-ee58-45b0-a464-c40477d23da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7f35c-2a8a-4498-b4ff-fdaa2a4c2a2b",
   "metadata": {},
   "source": [
    "## Building the summarization chain\n",
    "\n",
    "Now, you'll combine your functions into a complete `summarization_chain` using the pipe operator `|`, which applies functions sequentially (similar to function composition where `f|g(x)` is equivalent to `f(g(x))`).\n",
    "\n",
    "The workflow follows these steps:\n",
    "1. Convert the input prompt to a HumanMessage\n",
    "2. Pass the message to LLM with tools\n",
    "3. Extract tool calls from LLM response\n",
    "4. Update message history with tool results\n",
    "5. Send updated messages back to LLM\n",
    "6. Repeat steps 3-5 as needed\n",
    "7. Finally, extract just the content from the final message using RunnableLambda\n",
    "\n",
    "Each step maintains state using RunnablePassthrough until you reach the final message, at which point you'll apply RunnableLambda to extract only the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f8b6fd7e-d438-4051-99d4-1d75167d73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_chain = (\n",
    "    # Start with initial query\n",
    "    RunnablePassthrough.assign(\n",
    "        messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    "    )\n",
    "    # First LLM call (extract video ID)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process first tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Update message history\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    "    )\n",
    "    # Second LLM call (fetch transcript)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process second tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages2=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Final message update\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    "    )\n",
    "    # Generate final summary\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    "    )\n",
    "    # Return just the summary text\n",
    "    | RunnableLambda(lambda x: x[\"summary\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0985b6-dfc0-4369-87e2-5f48a8a51c8e",
   "metadata": {},
   "source": [
    "Here's how you invoke the summarization chain with a YouTube video URL; this passes your query containing a YouTube URL to the chain, which automatically extracts the video ID, fetches the transcript, and generates a summary of the content.\n",
    "\n",
    "**Note: If you find any issues with the given video link below, try any Youtube video link of your choosing.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7ce15-bf34-4074-8b43-a64449d6cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "result = summarization_chain.invoke({\n",
    "    \"query\": \"Summarize this YouTube video: https://www.youtube.com/watch?v=t97ipSIDEfU\"\n",
    "})\n",
    "\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba773b-0c5b-405c-b2b4-1d1090949e66",
   "metadata": {},
   "source": [
    "---\n",
    "Up to this point, you've demonstrated how to manually orchestrate the tool calling process step by step. You first invoked the LLM with the user's query, interpreted its decision to use the `extract_video_id` tool, executed that tool, fed the result back to the LLM, processed its next decision to use the `fetch_transcript` tool, executed that tool, and finally had the LLM generate a summary based on the transcript.\n",
    "\n",
    "Now you'll see how to accomplish the same workflow more efficiently using LangChain's chain functionality, which automates this back-and-forth process of tool selection, execution, and response handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b15e6-a156-42d5-8838-685f3e40146b",
   "metadata": {},
   "source": [
    "#### Creating the initial message setup\n",
    "\n",
    "Here you're setting up the first step of your chain that will handle the initial user query. The `RunnablePassthrough.assign` creates a component that takes an input dictionary containing a \"query\" and converts it into a list containing a single `HumanMessage` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "22b2e1c3-40b4-494d-9f61-6be9792b7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_setup = RunnablePassthrough.assign(\n",
    "    messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b387e-f33c-4f02-a872-7ef8548443e7",
   "metadata": {},
   "source": [
    "#### Defining the first LLM interaction\n",
    "\n",
    "Here, you'll create the second step of your chain, which handles the first interaction with the language model. This component takes the formatted messages from the previous step, sends them to your tool-equipped LLM, and captures the response in a field called \"ai_response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fead2c5f-aa5e-4738-91e4-5ea9878a836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e8141-2092-4e6e-a330-a258219f0175",
   "metadata": {},
   "source": [
    "#### Processing the first tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's first tool call. This component:\n",
    "1. Executes each tool call by passing it to your `execute_tool` function, which runs the appropriate tool and returns the result as a `ToolMessage`\n",
    "2. Updates the message history by combining the original messages, the LLM's response, with the tool calls, and the tool results\n",
    "3. Prepares the updated conversation state for the next interaction with the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4ad04381-5448-4d8b-b69d-ff29122e6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65689a18-2065-4cdb-a86e-9d829a6a688a",
   "metadata": {},
   "source": [
    "#### Defining the second LLM interaction\n",
    "\n",
    "Here, you're creating the next step in your chain that handles the second interaction with the language model. This component takes the updated message history (which now includes the results from the first tool call) and sends it to the LLM again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67b3df10-65db-4ec4-90a0-a974eafaf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8f4db-861b-47b2-a1fb-2e3e9a15b7db",
   "metadata": {},
   "source": [
    "#### Processing the second tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's second tool call. Similar to the first tool processing step, this component executes the tool calls (typically fetching the transcript), creates tool messages with the results, and updates the message history by combining everything for the final summarization step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0b026f33-3426-40c9-8283-09258794d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages2=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8b2b6-f0ba-4035-b1bf-aa39287b13c8",
   "metadata": {},
   "source": [
    "#### Generating the final summary\n",
    "\n",
    "Here, you're defining the final step that produces the summary of the YouTube video. This component:\n",
    "1. Takes the complete message history (which now contains the original query, tool calls, and tool results)\n",
    "2. Invokes the LLM one last time to generate a summary\n",
    "3. Extracts just the content field from the LLM's response\n",
    "4. Uses a RunnableLambda to return only the summary text as the final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8b828b24-962e-4f9e-a29e-234cfc93b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = RunnablePassthrough.assign(\n",
    "    summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    ") | RunnableLambda(lambda x: x[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a2429-18bf-4c8d-a1da-c448cc02412f",
   "metadata": {},
   "source": [
    "#### Assembling the complete chain\n",
    "\n",
    "Now, you're combining all the individual components you've defined into a single cohesive chain. By piping each step to the next, you'll create a workflow that:\n",
    "1. Formats the initial query\n",
    "2. Gets the first LLM response (video ID extraction)\n",
    "3. Processes the first tool call\n",
    "4. Gets the second LLM response (transcript request)\n",
    "5. Processes the second tool call\n",
    "6. Generates the final summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1cffc298-ac27-49f9-8630-4c687f557b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    initial_setup\n",
    "    | first_llm_call\n",
    "    | first_tool_processing\n",
    "    | second_llm_call\n",
    "    | second_tool_processing\n",
    "    | final_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ec552-a840-4bf3-8711-43d21241e072",
   "metadata": {},
   "source": [
    "Now, you're testing your automated chain with the original video summarization query you handled manually before. By passing in the same query to your chain, you can confirm that it produces the same results but in a much more streamlined manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0d826022-88ec-4a5c-a19d-d11177815087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The YouTube video explains **Retrieval-Augmented Generation (RAG)**, a framework to enhance the accuracy and reliability of large language models (LLMs). Key points include:\n",
      "\n",
      "1. **LLM Challenges**:  \n",
      "   - **Outdated Information**: LLMs rely on training data, which may not reflect the latest discoveries (e.g., the number of moons orbiting planets).  \n",
      "   - **Lack of Sourcing**: LLMs often provide answers without citing verifiable sources, increasing the risk of \"hallucinations.\"\n",
      "\n",
      "2. **RAG Framework**:  \n",
      "   - Combines **retrieval** (searching external content stores like databases or the internet) with **generative models** (LLMs).  \n",
      "   - Before answering, the model **retrieves relevant, up-to-date information** from a trusted source (e.g., NASA data). This ensures answers are evidence-backed and current.  \n",
      "\n",
      "3. **Example**:  \n",
      "   - When asked, \"Which planet has the most moons?\" an LLM might incorrectly say *Jupiter* (based on outdated training data). RAG retrieves the latest data, revealing *Saturn* has more moons (146 vs. Jupiter's 88).  \n",
      "\n",
      "4. **Benefits of RAG**:  \n",
      "   - Reduces hallucinations by grounding answers in primary sources.  \n",
      "   - Allows models to say \"I donâ€™t know\" if the content store lacks sufficient information.  \n",
      "   - Simplifies updates: Instead of retraining models, simply update the content store with new data.  \n",
      "\n",
      "5. **Challenges**:  \n",
      "   - Requires high-quality retrieval systems to find the best evidence.  \n",
      "   - Balancing retrieval efficiency with generative accuracy is an active research area.  \n",
      "\n",
      "The video concludes by emphasizing RAGâ€™s potential to make LLMs more trustworthy and adaptable in dynamic fields like science.\n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"}\n",
    "result = summarization_chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91154084-0dbd-4356-8114-4de89fde1654",
   "metadata": {},
   "source": [
    "#### Testing the Chain with a Different Query\n",
    "\n",
    "Here, you're testing your completed chain with a new query to demonstrate its flexibility. Instead of requesting a video summary, you're asking for information about trending videos in India. You'll create a dictionary with the query and invoke your chain, which will handle all the necessary tool calls automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33992e7-49cd-4c3a-ba08-0d05f7f680a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL NOT WORK AS TRENDS TOOL IS NOT WORKING\n",
    "\n",
    "# query = {\"query\": \"Get top 3 youtube videos in India and their metadata\"}\n",
    "# result = summarization_chain.invoke(query)\n",
    "# print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20813ed-499a-49ad-bf79-173935d46a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf5a90-8229-47aa-92ec-7b29cdc8d809",
   "metadata": {},
   "source": [
    "## Recursive chain flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09ede0-36f9-4e84-8a32-1f009c5bf31c",
   "metadata": {},
   "source": [
    "Now that you've created a chain that works well for your specific two-step tool calling process, you need to consider more complex scenarios. Your current chain is limited to exactly two tool calls in a fixed sequence. In real-world applications, you might need a variable number of tool calls depending on the user's query - for example, getting trending videos and then fetching metadata for each video, or searching for videos on a topic and then getting transcripts for multiple results.\n",
    "\n",
    "To handle these more complex scenarios, you'll build a recursive chain that can dynamically decide how many tool calls are needed and continue processing until all necessary information has been gathered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "44e071a9-f07c-47d1-a833-883b55b05381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "import json\n",
    "\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        content = json.dumps(result) if isinstance(result, (dict, list)) else str(result)\n",
    "    except Exception as e:\n",
    "        content = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return ToolMessage(\n",
    "        content=content,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150eb535-a855-48e9-b09b-2d337e5d8d7f",
   "metadata": {},
   "source": [
    "#### Defining the core processing logic\n",
    "\n",
    "This function handles the core processing logic of your recursive chain. It takes the current conversation history and:\n",
    "\n",
    "1. Identifies the most recent message in the conversation\n",
    "2. Extracts all tool calls from that message and executes them in parallel using your `execute_tool` helper\n",
    "3. Updates the message history by adding the tool response messages\n",
    "4. Gets the next response from the language model based on the updated conversation\n",
    "5. Returns the complete updated message history with both tool responses and the new LLM response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c7630e99-7156-480b-993c-b8716ebeb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tool_calls(messages):\n",
    "    \"\"\"Recursive tool call processor\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Execute all tool calls in parallel\n",
    "    tool_messages = [\n",
    "        execute_tool(tc) \n",
    "        for tc in getattr(last_message, 'tool_calls', [])\n",
    "    ]\n",
    "    \n",
    "    # Add tool responses to message history\n",
    "    updated_messages = messages + tool_messages\n",
    "    \n",
    "    # Get next LLM response\n",
    "    next_ai_response = llm_with_tools.invoke(updated_messages)\n",
    "    \n",
    "    return updated_messages + [next_ai_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5649e5-e3e5-4666-981b-7cd07e2720aa",
   "metadata": {},
   "source": [
    "#### Creating the recursive stopping condition\n",
    "\n",
    "This function determines whether your recursive process should continue or terminate. It:\n",
    "\n",
    "1. Takes the current message history and examines the last message\n",
    "2. Checks if that message contains any tool calls using the `getattr` function (which safely handles cases where the attribute might not exist)\n",
    "3. Returns a boolean value - `True` if there are more tool calls to process, and `False` when you reach a point where the LLM has provided a final answer without requesting additional tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9ac9ad3d-552f-4d0e-8c15-5f276cdc90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(messages):\n",
    "    \"\"\"Check if you need another iteration\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    return bool(getattr(last_message, 'tool_calls', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb34cc-3079-4a08-851c-20b329fea1c2",
   "metadata": {},
   "source": [
    "\n",
    "#### Implementing the recursive function\n",
    "\n",
    "This function implements the actual recursion that powers your dynamic tool calling process:\n",
    "\n",
    "1. It first checks the stopping condition using the `should_continue` function to determine if more tools need to be called\n",
    "2. If more tool calls are needed, it processes those calls using your `process_tool_calls` function and then recursively calls itself with the updated messages\n",
    "3. If no more tool calls are needed, it returns the final message history, which contains the complete conversation, including the LLM's final response\n",
    "\n",
    "After defining this recursive function, you'll wrap it in a `RunnableLambda` to make it compatible with LangChain's chain architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bed3e048-9b47-4c08-9054-eb9a8ce131d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_chain(messages):\n",
    "    \"\"\"Recursively process tool calls until completion\"\"\"\n",
    "    if should_continue(messages):\n",
    "        new_messages = process_tool_calls(messages)\n",
    "        return _recursive_chain(new_messages)\n",
    "    return messages\n",
    "\n",
    "recursive_chain = RunnableLambda(_recursive_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2334af-5df1-43e0-b0c8-e6c1dc9e5239",
   "metadata": {},
   "source": [
    "#### Building the complete universal chain\n",
    "\n",
    "Now, you're assembling your final universal chain that can handle any type of query requiring any number of tool calls. This chain consists of three main steps:\n",
    "\n",
    "1. The first step converts the user query into a properly formatted `HumanMessage` object\n",
    "2. The second step sends this initial message to your tool-equipped LLM and adds the LLM's first response to the message history\n",
    "3. The final step passes the conversation to your recursive chain, which will handle all subsequent tool calls until the LLM provides a final answer\n",
    "\n",
    "This universal chain is much more flexible than your earlier fixed-step chain, as it can dynamically adapt to queries that require different numbers and types of tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89fe137e-7b64-4011-ad86-71c64dbb7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_chain = (\n",
    "    RunnableLambda(lambda x: [HumanMessage(content=x[\"query\"])])\n",
    "    | RunnableLambda(lambda messages: messages + [llm_with_tools.invoke(messages)])\n",
    "    | recursive_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a935678-f47a-41d3-9232-0a1808e60eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s a summary of the YouTube video \"Retrieval-Augmented Generation (RAG) Explained\" by Marina Danilevsky:\\n\\n**Key Concepts:**\\n1. **LLM Challenges** - Large language models can produce outdated information and lack verifiable sources (e.g., incorrectly stating Jupiter has 88 moons when Saturn actually has 146).\\n\\n2. **RAG Framework** - Combines retrieval and generation:\\n   - **Retrieval** - Queries external knowledge sources (like NASA\\'s database) for up-to-date information\\n   - **Generation** - Uses retrieved data to produce evidence-backed responses\\n\\n3. **Benefits**:\\n   - Ensures answers are current (e.g., automatically updates moon counts as discoveries happen)\\n   - Reduces hallucinations by grounding responses in verified sources\\n   - Enables \"I don\\'t know\" responses for unanswerable queries\\n   - Provides traceable evidence for conclusions\\n\\n4. **Implementation** - The prompt structure becomes: [Instruction] + [Retrieved Content] + [User Question] â†’ [Evidence-Based Response]\\n\\n**Key Takeaway**: RAG addresses LLM limitations by creating a feedback loop between dynamic knowledge sources and language generation, making AI responses more accurate and accountable. The talk emphasizes ongoing research to improve both retrieval accuracy and generative quality.' additional_kwargs={'reasoning_content': \"Okay, the user wants a summary of the YouTube video with the ID T-D1OfcDW1M in English. I've already extracted the video ID and fetched the transcript. Now, I need to create a concise summary based on the provided transcript.\\n\\nFirst, I'll read through the transcript to understand the main points. The speaker, Marina Danilevsky, discusses Retrieval-Augmented Generation (RAG) as a framework to improve large language models (LLMs). She uses an example about planets and their moons to illustrate how LLMs can be outdated and lack sources. She explains that RAG addresses these issues by retrieving up-to-date information from a content store before generating a response. The key benefits mentioned are accuracy and the ability to provide evidence, reducing hallucinations.\\n\\nI need to structure the summary to highlight the problem with LLMs, introduce RAG, explain how it works, and outline its benefits. I should make sure to mention the example she used to make it relatable. Also, note the importance of both improving retrieval and generation components. Keep it clear and concise, avoiding technical jargon where possible. Let me put that all together in a coherent way.\\n\"} response_metadata={'token_usage': {'completion_tokens': 515, 'prompt_tokens': 2190, 'total_tokens': 2705, 'completion_time': 1.2670341569999999, 'prompt_time': 0.076284625, 'queue_time': 0.099481857, 'total_time': 1.3433187819999999, 'prompt_tokens_details': {'cached_tokens': 512}}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--20a434fd-8582-4a21-96a6-15fa69bbc1cf-0' usage_metadata={'input_tokens': 2190, 'output_tokens': 515, 'total_tokens': 2705}\n"
     ]
    }
   ],
   "source": [
    "print(universal_chain.invoke({\n",
    "    \"query\": \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"\n",
    "})[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f63e6-8e16-47f4-9091-903bf4c77745",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2d7fd-07d6-4ec3-b4dd-d473d72d86e5",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a different video with a Youtube link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89206622-b7a5-4c9e-a1a8-9d77be6e5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714905a7-0962-43e1-a13a-1dd638fa7071",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = \"INSERT_VIDEO_ID_HERE\"  # Replace with the actual video ID\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3d6be-f782-46e9-a43c-3c82cec18cf6",
   "metadata": {},
   "source": [
    "### Exercise 2: Extract the video ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc939ee-6802-411e-96cf-629cd1cb2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bec23-76b4-42a0-931a-af6166fbf735",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = extract_video_id.run(youtube_url)\n",
    "print(f\"Extracted video ID: {video_id}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc338faf-ec63-4614-8a40-2f02cb9560b8",
   "metadata": {},
   "source": [
    "### Exercise 3: Collect all necessary data about the video in one go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063b75f-b6e0-416c-a593-d3b37c728118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcc613-b29c-4907-84ba-59e1484a75f8",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_metadata = get_full_metadata.run(youtube_url)\n",
    "print(f\"Retrieved metadata for: {video_metadata['title']}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58872de1-a077-4726-8050-fd84fd1ccdb1",
   "metadata": {},
   "source": [
    "### Exercise 4: Get video transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dd663-3f15-4035-8295-7434379a63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8c691-ec76-45bf-bc79-ac66a71fa2ad",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(f\"Retrieved transcript with {len(transcript)} characters\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc44f0c-f94c-4064-b562-0a199ed9d4c6",
   "metadata": {},
   "source": [
    "### Exercise 5: Get video thumbnails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b73b66-22f0-42c8-afd5-eaea1fcc8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d672c-7a65-48c9-a748-71bd04e075c6",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video thumbnails\n",
    "thumbnails = get_thumbnails.run(youtube_url)\n",
    "print(f\"Retrieved {len(thumbnails)} thumbnails\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4ee60-8e7a-4dab-8799-fa870483933c",
   "metadata": {},
   "source": [
    "### Let's have a comprehensive prompt to be passed to LLM to generate a summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd7a4c-1cfc-4639-bb90-e547dc515a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26673608-9994-48ff-b697-8ad20b52cdcf",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for prompt</summary>\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {video_metadata['title']}\n",
    "CHANNEL: {video_metadata['channel']}\n",
    "VIEWS: {video_metadata['views']}\n",
    "DURATION: {video_metadata['duration']} seconds\n",
    "LIKES: {video_metadata['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138cd36-7168-45ba-b938-20b21e6f1bd6",
   "metadata": {},
   "source": [
    "### Exercise 6: Single LLM invocation with all the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e03a10-a0f8-4f1b-9dd6-0356c51995a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3bd200-4601-4e0f-9c0c-bc11136fbd76",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0e39f-85b4-4f6a-a9e5-1857aa997793",
   "metadata": {},
   "source": [
    "### Exercise 7: Display the comprehensive analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e6285-7136-4af2-bba1-f4aa79bd6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5662b4a-b07b-4230-adfc-2480e38f5429",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "print(\"\\n===== VIDEO ANALYSIS =====\\n\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_experimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "prev_pub_hash": "1603f145970a8c2283bae36941000a6d4b9e191c2755a7412e95925fa9d1ae18"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
