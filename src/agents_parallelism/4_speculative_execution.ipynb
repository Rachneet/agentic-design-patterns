{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d62d19",
   "metadata": {},
   "source": [
    "### Speculative Execution & Pre-fetching\n",
    "\n",
    "#### Introduction: The Art of Anticipation for a Faster AI\n",
    "This notebook explores a sophisticated parallelism pattern designed to dramatically reduce perceived latency in interactive AI systems: Speculative Execution & Pre-fetching. The principle is to anticipate the agent's most likely next actionâ€”usually a slow, data-gathering tool callâ€”and begin executing it in parallel with the agent's primary reasoning process.\n",
    "\n",
    "#### Why is this pattern so impactful?\n",
    "In many agentic workflows, the sequence is: User Input -> Agent Thinks (LLM call) -> Agent Acts (Tool call). The user waits during both the thinking and acting phases. Speculative execution overlaps these two phases. While the agent is thinking, the system makes an educated guess about the upcoming action and starts it. If the guess is correct, the tool call's latency is effectively hidden behind the LLM's inference time, making the agent feel instantaneous.\n",
    "\n",
    "#### Role in a Large-Scale System: Creating Proactive & Hyper-Responsive User Experiences\n",
    "This is a key architectural pattern for any high-throughput, user-facing system where responsiveness is a primary feature. It's the difference between an AI that feels reactive and one that feels proactive and intelligent.\n",
    "\n",
    "- Customer Support Chatbots: Pre-fetching user account details and recent orders the moment a chat begins.\n",
    "- Data Analysis Tools: Speculatively running a common default query on a dashboard as soon as it's loaded.\n",
    "- Code Assistants: Pre-fetching relevant documentation for a function as the developer is typing its name.\n",
    "\n",
    "We will build a customer support agent that speculatively fetches a user's order history, demonstrating how this pattern can eliminate tool-call latency from the user's perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94a94d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../../figures/speculative_execution.png\" width=\"1000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98e3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure LangSmith for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Industrial - Speculative Execution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3d3d418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = ChatHuggingFace(\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        model=\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3672c2",
   "metadata": {},
   "source": [
    "#### Show that a tool is really slow in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a8776bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import time\n",
    "import json\n",
    "\n",
    "DATABASE_LATENCY_SECONDS = 3\n",
    "\n",
    "@tool\n",
    "def get_order_history(user_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the order history for a given user. \n",
    "    \n",
    "    Use this tool when the user asks about:\n",
    "    - Their orders\n",
    "    - Order status\n",
    "    - Recent purchases\n",
    "    - Delivery status\n",
    "    - Purchase history\n",
    "    \n",
    "    Args:\n",
    "        user_id: The unique identifier of the user\n",
    "    \"\"\"\n",
    "    print(f\"--- [DATABASE] Starting query for user_id: {user_id}. This will take {DATABASE_LATENCY_SECONDS} seconds. ---\")\n",
    "    time.sleep(DATABASE_LATENCY_SECONDS)\n",
    "    \n",
    "    # Mock data for demonstration\n",
    "    mock_db = {\n",
    "        \"user123\": [\n",
    "            {\"order_id\": \"A123\", \"item\": \"QuantumLeap AI Processor\", \"status\": \"Shipped\"},\n",
    "            {\"order_id\": \"B456\", \"item\": \"Smart Coffee Mug\", \"status\": \"Delivered\"}\n",
    "        ]\n",
    "    }\n",
    "    result = mock_db.get(user_id, [])\n",
    "    print(f\"--- [DATABASE] Query finished for user_id: {user_id}. ---\")\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "654d1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_order_history]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0e1429eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from concurrent.futures import Future\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_id: str\n",
    "    # 'prefetched_data' will hold a Python 'Future' object, representing the background tool call.\n",
    "    prefetched_data: Optional[Future]\n",
    "    # 'agent_decision' will hold the actual tool call that the LLM decides to make.\n",
    "    agent_decision: Optional[BaseMessage]\n",
    "    performance_log: Annotated[List[str], add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf856a0a",
   "metadata": {},
   "source": [
    "The key part of this GraphState is the prefetched_data: Optional[Future] field. A Future is a standard Python object that acts as a placeholder for a result that is not yet available. This allows our entry point node to return immediately after starting the background task, while the Future object is passed through the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d6677e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# We create a thread pool to run our background tasks.\n",
    "thread_pool = ThreadPoolExecutor(max_workers=5)\n",
    "def entry_point(state: GraphState):\n",
    "    \"\"\"The entry point node: starts the speculative pre-fetch and the main agent reasoning in parallel.\"\"\"\n",
    "    print(\"--- [ORCHESTRATOR] Entry point started. --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Start the speculative pre-fetch in a background thread using our thread pool.\n",
    "    #    The .submit() method returns a 'Future' object immediately.\n",
    "    print(\"--- [ORCHESTRATOR] Starting speculative pre-fetch of order history... ---\")\n",
    "    prefetched_data_future = thread_pool.submit(get_order_history.invoke, {\"user_id\": state['user_id']})\n",
    "    \n",
    "    # 2. In parallel, while the tool is running in the background, we start the main agent's LLM call.\n",
    "    print(\"--- [ORCHESTRATOR] Starting main agent LLM call... ---\")\n",
    "    agent_response = llm_with_tools.invoke(state['messages'])\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[Orchestrator] LLM reasoning completed in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # The node returns the Future object and the agent's decision to be added to the state.\n",
    "    return {\n",
    "        \"prefetched_data\": prefetched_data_future,\n",
    "        \"agent_decision\": agent_response,\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eaf2f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Node 2: Tool Executor (with pre-fetch checking)\n",
    "def tool_executor_node(state: GraphState):\n",
    "    \"\"\"Executes the agent's chosen tool, leveraging pre-fetched data if available.\"\"\"\n",
    "    print(\"--- [TOOL EXECUTOR] Node started. --- \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    agent_decision = state[\"agent_decision\"]\n",
    "    tool_call = agent_decision.tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_args = tool_call[\"args\"]\n",
    "\n",
    "    # tool invocation\n",
    "    # Check if the desired tool call matches our speculation\n",
    "    if tool_name == \"get_order_history\":\n",
    "        print(\"--- [TOOL EXECUTOR] Agent wants order history. Checking pre-fetch... ---\")\n",
    "        # Wait for the pre-fetch to complete and get the result\n",
    "        prefetched_future = state['prefetched_data']\n",
    "        tool_result = prefetched_future.result()\n",
    "        print(\"--- [TOOL EXECUTOR] Pre-fetch successful! Using cached data instantly. ---\")\n",
    "    else:\n",
    "        # If the agent wants a different tool, we would execute it normally here\n",
    "        print(f\"--- [TOOL EXECUTOR] Agent wants a different tool ({tool_name}). Executing normally. ---\")\n",
    "        # For this demo, we'll assume only get_order_history exists\n",
    "        tool_result = \"Tool not implemented for this demo.\"\n",
    "    \n",
    "    tool_message = ToolMessage(\n",
    "        content=tool_result,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "    # Note: This time represents how long it took to get the result from this node's perspective\n",
    "    log_entry = f\"[ToolExecutor] Resolved tool call in {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [agent_decision, tool_message],\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0150f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: Final Answer Synthesizer\n",
    "def final_answer_node(state: GraphState):\n",
    "    \"\"\"Generates the final response to the user.\"\"\"\n",
    "    print(\"--- [SYNTHESIZER] Generating final answer... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # We need to remove the pre-fetched data from the state before the final LLM call\n",
    "    # as it's not serializable and not part of the message history.\n",
    "    final_state_messages = state['messages']\n",
    "    final_response = llm.invoke(final_state_messages)\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    log_entry = f\"[Synthesizer] Final LLM call took {execution_time:.2f}s.\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [final_response],\n",
    "        \"performance_log\": [log_entry]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac521e",
   "metadata": {},
   "source": [
    "### Defining Graph Edges and Assembling the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "70fcb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def should_call_tool(state: GraphState) -> str:\n",
    "    if state['agent_decision'].tool_calls:\n",
    "        return \"execute_tool\"\n",
    "    return \"final_answer\"  # Route to final_answer instead of END when no tools\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"entry_point\", entry_point)\n",
    "workflow.add_node(\"execute_tool\", tool_executor_node)\n",
    "workflow.add_node(\"final_answer\", final_answer_node)\n",
    "\n",
    "# Build the graph\n",
    "workflow.set_entry_point(\"entry_point\")\n",
    "\n",
    "# Add conditional edges with explicit mapping\n",
    "workflow.add_conditional_edges(\n",
    "    \"entry_point\", \n",
    "    should_call_tool,\n",
    "    {\n",
    "        \"execute_tool\": \"execute_tool\",\n",
    "        \"final_answer\": \"final_answer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"execute_tool\", \"final_answer\")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7613cabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6c454cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [ORCHESTRATOR] Entry point started. --- \n",
      "--- [ORCHESTRATOR] Starting speculative pre-fetch of order history... ---\n",
      "--- [ORCHESTRATOR] Starting main agent LLM call... ---\n",
      "--- [DATABASE] Starting query for user_id: user123. This will take 3 seconds. ---\n",
      "[Orchestrator] LLM reasoning completed in 1.04s.\n",
      "--- [TOOL EXECUTOR] Node started. --- \n",
      "--- [TOOL EXECUTOR] Agent wants order history. Checking pre-fetch... ---\n",
      "--- [DATABASE] Query finished for user_id: user123. ---\n",
      "--- [TOOL EXECUTOR] Pre-fetch successful! Using cached data instantly. ---\n",
      "[ToolExecutor] Resolved tool call in 1.97s.\n",
      "--- [SYNTHESIZER] Generating final answer... ---\n",
      "[Synthesizer] Final LLM call took 1.55s.\n",
      "\n",
      "==================================================\n",
      "FINAL ANSWER:\n",
      "==================================================\n",
      "Here's the status of your recent orders:\n",
      "\n",
      "- **Order A123**: QuantumLeap AI Processor â€“ **Shipped**  \n",
      "- **Order B456**: Smart Coffee Mug â€“ **Delivered**\n",
      "\n",
      "Let me know if you need any further details or assistance! ðŸ˜Š\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE LOG:\n",
      "==================================================\n",
      "  â€¢ content='[Orchestrator] LLM reasoning completed in 1.04s.' additional_kwargs={} response_metadata={} id='6a761641-9eb3-4f7f-b73b-f520e7fd88ab'\n",
      "  â€¢ content='[ToolExecutor] Resolved tool call in 1.97s.' additional_kwargs={} response_metadata={} id='f17760e9-50b9-4b89-86c7-fe3fcc63cb6a'\n",
      "  â€¢ content='[Synthesizer] Final LLM call took 1.55s.' additional_kwargs={} response_metadata={} id='5bea9b6d-6a04-478b-9ae1-a6d5197f577c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [DATABASE] Starting query for user_id: user123. This will take 3 seconds. ---\n",
      "--- [DATABASE] Query finished for user_id: user123. ---\n",
      "--- [DATABASE] Starting query for user_id: user123. This will take 3 seconds. ---\n",
      "--- [DATABASE] Query finished for user_id: user123. ---\n",
      "--- [DATABASE] Starting query for user_id: user123. This will take 3 seconds. ---\n",
      "--- [DATABASE] Query finished for user_id: user123. ---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"Hi, can you tell me the status of my recent orders?\")],\n",
    "    \"user_id\": \"user123\"\n",
    "}\n",
    "\n",
    "# Stream with 'values' mode to get the full state after each node\n",
    "final_state = None\n",
    "for state in app.stream(inputs, stream_mode=\"values\"):\n",
    "    final_state = state\n",
    "\n",
    "# Extract the final answer\n",
    "final_answer = final_state[\"messages\"][-1].content\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\" * 50)\n",
    "print(final_answer)\n",
    "\n",
    "# Get performance logs\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE LOG:\")\n",
    "print(\"=\" * 50)\n",
    "for log in final_state.get(\"performance_log\", []):\n",
    "    print(f\"  â€¢ {log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bfbfe05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='[Orchestrator] LLM reasoning completed in 1.04s.', additional_kwargs={}, response_metadata={}, id='6a761641-9eb3-4f7f-b73b-f520e7fd88ab'), HumanMessage(content='[ToolExecutor] Resolved tool call in 1.97s.', additional_kwargs={}, response_metadata={}, id='f17760e9-50b9-4b89-86c7-fe3fcc63cb6a'), HumanMessage(content='[Synthesizer] Final LLM call took 1.55s.', additional_kwargs={}, response_metadata={}, id='5bea9b6d-6a04-478b-9ae1-a6d5197f577c')]\n",
      "1.04s\n"
     ]
    }
   ],
   "source": [
    "print(final_state[\"performance_log\"])\n",
    "print(final_state[\"performance_log\"][0].content.split(\" \")[-1].rstrip(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d89cc7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXECUTION TRACE\n",
      "================================================================================\n",
      "--- [ORCHESTRATOR] Entry point started. --- \n",
      "--- [ORCHESTRATOR] Starting speculative pre-fetch of order history... ---\n",
      "--- [ORCHESTRATOR] Starting main agent LLM call... ---\n",
      "[Orchestrator] LLM reasoning completed in 1.36s.\n",
      "\n",
      "--- Step 1: ENTRY_POINT ---\n",
      "  Log: [Orchestrator] LLM reasoning completed in 1.36s.\n",
      "--- [TOOL EXECUTOR] Node started. --- \n",
      "--- [TOOL EXECUTOR] Agent wants order history. Checking pre-fetch... ---\n",
      "--- [TOOL EXECUTOR] Pre-fetch successful! Using cached data instantly. ---\n",
      "[ToolExecutor] Resolved tool call in 1.64s.\n",
      "\n",
      "--- Step 2: EXECUTE_TOOL ---\n",
      "  Message type: AIMessage\n",
      "  Message type: ToolMessage\n",
      "  Log: [ToolExecutor] Resolved tool call in 1.64s.\n",
      "--- [SYNTHESIZER] Generating final answer... ---\n",
      "[Synthesizer] Final LLM call took 1.68s.\n",
      "\n",
      "--- Step 3: FINAL_ANSWER ---\n",
      "  Message type: AIMessage\n",
      "  Log: [Synthesizer] Final LLM call took 1.68s.\n",
      "\n",
      "================================================================================\n",
      "FINAL ANSWER\n",
      "================================================================================\n",
      "Hereâ€™s the status of your recent orders:\n",
      "\n",
      "- **Order ID: A123**  \n",
      "  Item: QuantumLeap AI Processor  \n",
      "  Status: Shipped  \n",
      "\n",
      "- **Order ID: B456**  \n",
      "  Item: Smart Coffee Mug  \n",
      "  Status: Delivered  \n",
      "\n",
      "Let me know if you need further assistance! ðŸ˜Š\n",
      "\n",
      "================================================================================\n",
      "ALL PERFORMANCE LOGS\n",
      "================================================================================\n",
      "  â€¢ [Orchestrator] LLM reasoning completed in 1.36s.\n",
      "  â€¢ [ToolExecutor] Resolved tool call in 1.64s.\n",
      "  â€¢ [Synthesizer] Final LLM call took 1.68s.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"Hi, can you tell me the status of my recent orders?\")],\n",
    "    \"user_id\": \"user123\"\n",
    "}\n",
    "\n",
    "# Track updates from each node\n",
    "all_messages = []\n",
    "all_logs = []\n",
    "step_counter = 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTION TRACE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for output in app.stream(inputs, stream_mode=\"updates\"):\n",
    "    node_name = list(output.keys())[0]\n",
    "    node_output = output[node_name]\n",
    "    \n",
    "    print(f\"\\n--- Step {step_counter}: {node_name.upper()} ---\")\n",
    "    \n",
    "    # Collect messages if present\n",
    "    if \"messages\" in node_output:\n",
    "        for msg in node_output[\"messages\"]:\n",
    "            all_messages.append(msg)\n",
    "            print(f\"  Message type: {type(msg).__name__}\")\n",
    "    \n",
    "    # Collect performance logs if present\n",
    "    if \"performance_log\" in node_output:\n",
    "        all_logs.extend(node_output[\"performance_log\"])\n",
    "        for log in node_output[\"performance_log\"]:\n",
    "            print(f\"  Log: {log}\")\n",
    "    \n",
    "    step_counter += 1\n",
    "\n",
    "# The final message is the answer\n",
    "final_answer = all_messages[-1].content\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL ANSWER\")\n",
    "print(\"=\" * 80)\n",
    "print(final_answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL PERFORMANCE LOGS\")\n",
    "print(\"=\" * 80)\n",
    "for log in all_logs:\n",
    "    print(f\"  â€¢ {log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5d62f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [ORCHESTRATOR] Entry point started. --- \n",
      "--- [ORCHESTRATOR] Starting speculative pre-fetch of order history... ---\n",
      "--- [ORCHESTRATOR] Starting main agent LLM call... ---\n",
      "[Orchestrator] LLM reasoning completed in 1.25s.\n",
      "--- [TOOL EXECUTOR] Node started. --- \n",
      "--- [TOOL EXECUTOR] Agent wants order history. Checking pre-fetch... ---\n",
      "--- [TOOL EXECUTOR] Pre-fetch successful! Using cached data instantly. ---\n",
      "[ToolExecutor] Resolved tool call in 1.75s.\n",
      "--- [SYNTHESIZER] Generating final answer... ---\n",
      "[Synthesizer] Final LLM call took 1.63s.\n",
      "==================================================\n",
      "FINAL ANSWER\n",
      "==================================================\n",
      "Here is the status of your recent orders:\n",
      "\n",
      "- **Order ID: A123** â€“ QuantumLeap AI Processor â€“ **Shipped**  \n",
      "- **Order ID: B456** â€“ Smart Coffee Mug â€“ **Delivered**\n",
      "\n",
      "Let me know if you need further details or assistance! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# the simplest way without streaming\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"Hi, can you tell me the status of my recent orders?\")],\n",
    "    \"user_id\": \"user123\"\n",
    "}\n",
    "\n",
    "# Get final state directly\n",
    "final_state = app.invoke(inputs)\n",
    "\n",
    "# Access the final answer\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL ANSWER\")\n",
    "print(\"=\" * 50)\n",
    "print(final_state[\"messages\"][-1].content)  # Last message is usually the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c39ed",
   "metadata": {},
   "source": [
    "### Performance Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f3aa0c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "                  PERFORMANCE SHOWDOWN\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "             SPECULATIVE EXECUTION WORKFLOW (Our Run)\n",
      "------------------------------------------------------------\n",
      "1. Agent Thinks (LLM Call 1):       1.25 seconds\n",
      "   (Database Query: 3.00s ran in parallel, fully hidden)\n",
      "2. Tool Result Resolution:          1.25 seconds (Instant cache hit)\n",
      "3. Synthesize Answer (LLM Call 2):  1.25 seconds\n",
      "------------------------------------------------------------\n",
      "Total Time to Final Answer: 3.75 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "             TRADITIONAL SEQUENTIAL WORKFLOW (Simulated)\n",
      "------------------------------------------------------------\n",
      "1. Agent Thinks (LLM Call 1):       1.25 seconds\n",
      "2. Execute Tool (Database Query):   3.00 seconds (User waits)\n",
      "3. Synthesize Answer (LLM Call 2):  1.25 seconds\n",
      "------------------------------------------------------------\n",
      "Simulated Total Time: 5.50 seconds\n",
      "\n",
      "\n",
      "============================================================\n",
      "                        CONCLUSION\n",
      "============================================================\n",
      "Time Saved: 1.75 seconds\n",
      "Perceived Latency Reduction: 32%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the timing data from the final_state's performance log after a full run.\n",
    "# (a full run has been completed and final_state is populated)\n",
    "llm_time_1 = float(final_state[\"performance_log\"][0].content.split(\" \")[-1].rstrip(\".\").replace(\"s\", \"\"))\n",
    "resolution_time = float(final_state[\"performance_log\"][0].content.split(\" \")[-1].rstrip(\".\").replace(\"s\", \"\"))\n",
    "llm_time_2 = float(final_state[\"performance_log\"][0].content.split(\" \")[-1].rstrip(\".\").replace(\"s\", \"\"))\n",
    "db_time = DATABASE_LATENCY_SECONDS # Our known latency\n",
    "\n",
    "# Calculate the total time for our speculative run.\n",
    "speculative_total = llm_time_1 + resolution_time + llm_time_2\n",
    "# Calculate the total time for a simulated sequential run.\n",
    "sequential_total = llm_time_1 + db_time + llm_time_2\n",
    "time_saved = sequential_total - speculative_total\n",
    "reduction_percent = (time_saved / sequential_total) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                  PERFORMANCE SHOWDOWN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"             SPECULATIVE EXECUTION WORKFLOW (Our Run)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"1. Agent Thinks (LLM Call 1):       {llm_time_1:.2f} seconds\")\n",
    "print(f\"   (Database Query: {db_time:.2f}s ran in parallel, fully hidden)\")\n",
    "print(f\"2. Tool Result Resolution:          {resolution_time:.2f} seconds (Instant cache hit)\")\n",
    "print(f\"3. Synthesize Answer (LLM Call 2):  {llm_time_2:.2f} seconds\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Total Time to Final Answer: {speculative_total:.2f} seconds\\n\\n\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"             TRADITIONAL SEQUENTIAL WORKFLOW (Simulated)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"1. Agent Thinks (LLM Call 1):       {llm_time_1:.2f} seconds\")\n",
    "print(f\"2. Execute Tool (Database Query):   {db_time:.2f} seconds (User waits)\")\n",
    "print(f\"3. Synthesize Answer (LLM Call 2):  {llm_time_2:.2f} seconds\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Simulated Total Time: {sequential_total:.2f} seconds\\n\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"                        CONCLUSION\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Time Saved: {time_saved:.2f} seconds\")\n",
    "print(f\"Perceived Latency Reduction: {reduction_percent:.0f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92cc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_experimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
