{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018a963a",
   "metadata": {},
   "source": [
    "### Sharded & Scattered Retrieval\n",
    "As a knowledge base grows from thousands to millions or billions of documents …\n",
    "\n",
    "A single, monolithic vector store becomes a major bottleneck. Search latency increases, and the index becomes unwieldy to manage and update.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../figures/shard_retrieval.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c164657",
   "metadata": {},
   "source": [
    "The architectural solution for this issue is Sharded & Scattered Retrieval. The core idea is that, instead of having one massive index, we partition (or shard) our knowledge base into multiple, smaller, independent vector stores.\n",
    "\n",
    "These shards can be organized by any logical division, such as topic, date, or data source. When a user query arrives, a central orchestrator “scatters” the query to all shards, which perform their searches in parallel. The results are then gathered and re-ranked to find the globally best documents.\n",
    "\n",
    "We will build a simulated two-shard system (Engineering vs. Marketing) and compare it to a monolithic system to understand the benefits in both latency and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bc54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c089e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachneet/projects/agents_experimental/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = ChatHuggingFace(\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        model=\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41000fb1",
   "metadata": {},
   "source": [
    "### Creating the Knowledge Base Shards\n",
    "We will create two separate vector stores to simulate our shards. Each will contain domain-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78e1a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nm/srw3ygl55jbgpddqwgysqjpm0000gn/T/ipykernel_12576/3009547170.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base shards created: Engineering KB (3 docs), Marketing KB (3 docs).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Engineering KB Documents\n",
    "eng_docs = [\n",
    "    Document(page_content=\"The QuantumLeap V3 processor utilizes a 3nm process node and features a dedicated AI accelerator core with 128 tensor units. API endpoint `/api/v3/status` provides real-time thermal throttling data.\", metadata={\"source\": \"eng-kb\"}),\n",
    "    Document(page_content=\"Firmware update v2.1 for the Aura Smart Ring optimizes the photoplethysmography (PPG) sensor algorithm for more accurate sleep stage detection. The update is deployed via the mobile app.\", metadata={\"source\": \"eng-kb\"}),\n",
    "    Document(page_content=\"The Smart Mug's heating element is a nickel-chromium coil controlled by a PID controller. It maintains temperature within +/- 1 degree Celsius. Battery polling is done via the `getBattery` function.\", metadata={\"source\": \"eng-kb\"})\n",
    "]\n",
    "\n",
    "# Marketing KB Documents\n",
    "mkt_docs = [\n",
    "    Document(page_content=\"Press Release: Unveiling the QuantumLeap V3, the AI processor that redefines speed. 'It's a game-changer for creative professionals,' says CEO Jane Doe. Available Q4.\", metadata={\"source\": \"mkt-kb\"}),\n",
    "    Document(page_content=\"Product Page: The Aura Smart Ring is your personal wellness companion. Crafted from aerospace-grade titanium, it empowers you to unlock your full potential by understanding your body's signals.\", metadata={\"source\": \"mkt-kb\"}),\n",
    "    Document(page_content=\"Blog Post: 'Five Ways Our Smart Mug Supercharges Your Morning Routine.' The perfect temperature, from the first sip to the last, means your coffee is always perfect.\", metadata={\"source\": \"mkt-kb\"})\n",
    "]\n",
    "\n",
    "# Create embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the two vector store shards\n",
    "eng_vectorstore = FAISS.from_documents(eng_docs, embedding=embeddings)\n",
    "mkt_vectorstore = FAISS.from_documents(mkt_docs, embedding=embeddings)\n",
    "\n",
    "eng_retriever = eng_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "mkt_retriever = mkt_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(f\"Knowledge Base shards created: Engineering KB ({len(eng_docs)} docs), Marketing KB ({len(mkt_docs)} docs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734b9a4",
   "metadata": {},
   "source": [
    "### The Baseline - A Monolithic RAG System\n",
    "To establish a baseline, we'll first create a traditional RAG system with a single, combined knowledge base. We will add a simulated latency to its retrieval step to mimic searching a much larger index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7578fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 1. Create the monolithic vector store\n",
    "all_docs = eng_docs + mkt_docs\n",
    "monolithic_vectorstore = FAISS.from_documents(all_docs, embedding=embeddings)\n",
    "monolithic_retriever = monolithic_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 2. Simulate the increased latency of a large index\n",
    "def slow_retrieval(query):\n",
    "    print(\"--- [Monolithic Retriever] Searching large index... (simulating high latency) ---\")\n",
    "    time.sleep(2.5) # Simulate latency\n",
    "    return monolithic_retriever.invoke(query)\n",
    "\n",
    "slow_monolithic_retriever = RunnableLambda(slow_retrieval)\n",
    "\n",
    "# 3. Create the monolithic RAG chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert technical and marketing support agent. Answer the user's question based *only* on the provided context.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"[Source: {doc.metadata.get('source', 'N/A')}] {doc.page_content}\" for doc in docs)\n",
    "\n",
    "monolithic_rag_chain = (\n",
    "    {\"context\": slow_monolithic_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2cded",
   "metadata": {},
   "source": [
    "### Building the Sharded RAG Graph\n",
    "Now, let's build the superior, sharded system. The core of this system is a node that scatters the query to our two shards in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ded6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class ShardedRAGState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document]\n",
    "    final_answer: str\n",
    "\n",
    "# Node 1: Parallel Retrieval (Scatter-Gather)\n",
    "def parallel_retrieval_node(state: ShardedRAGState):\n",
    "    \"\"\"Scatters the query to all shards and gathers the results.\"\"\"\n",
    "    print(\"--- [Meta-Retriever] Scattering query to Engineering and Marketing shards in parallel... ---\")\n",
    "    \n",
    "    # We'll use a ThreadPool to run retrievals concurrently\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        # P_retrieval function to add a delay to each shard search\n",
    "        def p_retrieval(retriever):\n",
    "            time.sleep(0.5) # Simulate network hop and smaller index search time\n",
    "            return retriever.invoke(state['question'])\n",
    "        \n",
    "        futures = [executor.submit(p_retrieval, retriever) for retriever in [eng_retriever, mkt_retriever]]\n",
    "        \n",
    "        all_docs = []\n",
    "        for future in futures:\n",
    "            all_docs.extend(future.result())\n",
    "    \n",
    "    # In a real system, you'd add a re-ranking step here. For now, we'll just deduplicate.\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "    print(f\"--- [Meta-Retriever] Gathered {len(unique_docs)} unique documents from 2 shards. ---\")\n",
    "    return {\"retrieved_docs\": unique_docs}\n",
    "\n",
    "# Node 2: Generation Node (same as before)\n",
    "def generation_node(state: ShardedRAGState):\n",
    "    \"\"\"Synthesizes the final answer from the gathered documents.\"\"\"\n",
    "    print(\"--- [Generator] Synthesizing final answer... ---\")\n",
    "    context = format_docs(state['retrieved_docs'])\n",
    "    answer = (\n",
    "        generator_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    ).invoke({\"context\": context, \"question\": state['question']})\n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5698e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(ShardedRAGState)\n",
    "workflow.add_node(\"parallel_retrieval\", parallel_retrieval_node)\n",
    "workflow.add_node(\"generate_answer\", generation_node)\n",
    "\n",
    "workflow.set_entry_point(\"parallel_retrieval\")\n",
    "workflow.add_edge(\"parallel_retrieval\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "sharded_rag_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf293885",
   "metadata": {},
   "source": [
    "### Head-to-Head Comparison\n",
    "Now we will ask both systems a question that requires information from both the engineering and marketing knowledge bases to be answered completely and accurately. The monolithic system may struggle to find the less-dominant but still relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295097da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query has strong marketing keywords ('game-changer', 'creative professionals')\n",
    "# but also a specific technical question ('API status endpoint').\n",
    "user_query = \"I heard the new QuantumLeap V3 is a 'game-changer for creative professionals'. Can you tell me more about it, and is there an API endpoint to check its status?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050c89e",
   "metadata": {},
   "source": [
    "### Running the Monolithic RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f899a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [MONOLITHIC RAG] Starting run... ---\n",
      "--- [Monolithic Retriever] Searching large index... (simulating high latency) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "               MONOLITHIC RAG SYSTEM OUTPUT\n",
      "============================================================\n",
      "\n",
      "Retrieved Context:\n",
      "[Source: mkt-kb] Press Release: Unveiling the QuantumLeap V3, the AI processor that redefines speed. 'It's a game-changer for creative professionals,' says CEO Jane Doe. Available Q4.\n",
      "\n",
      "[Source: eng-kb] The QuantumLeap V3 processor utilizes a 3nm process node and features a dedicated AI accelerator core with 128 tensor units. API endpoint `/api/v3/status` provides real-time thermal throttling data.\n",
      "\n",
      "[Source: eng-kb] Firmware update v2.1 for the Aura Smart Ring optimizes the photoplethysmography (PPG) sensor algorithm for more accurate sleep stage detection. The update is deployed via the mobile app.\n",
      "\n",
      "[Source: mkt-kb] Product Page: The Aura Smart Ring is your personal wellness companion. Crafted from aerospace-grade titanium, it empowers you to unlock your full potential by understanding your body's signals.\n",
      "\n",
      "Final Answer:\n",
      "Yes, the QuantumLeap V3 is described as a \"game-changer for creative professionals\" by CEO Jane Doe, highlighting its significant performance improvements and capabilities tailored for creative workloads. It features a 3nm process node and includes a dedicated AI accelerator core with 128 tensor units, enabling faster and more efficient AI processing—beneficial for tasks like image rendering, video editing, and real-time creative tools.\n",
      "\n",
      "However, there is **no API endpoint specifically for the QuantumLeap V3** to check its status. The provided API endpoint `/api/v3/status` mentioned in the context is associated with the **Aura Smart Ring**, not the QuantumLeap V3 processor. Therefore, while the QuantumLeap V3 is powerful, real-time status monitoring via an API is not currently available for it.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [MONOLITHIC RAG] Starting run... ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# We'll capture the context to inspect it\n",
    "retrieved_context_mono = \"\"\n",
    "def capture_context_mono(docs):\n",
    "    global retrieved_context_mono\n",
    "    retrieved_context_mono = format_docs(docs)\n",
    "    return retrieved_context_mono\n",
    "\n",
    "monolithic_rag_chain_instrumented = (\n",
    "    {\"context\": slow_monolithic_retriever | capture_context_mono, \"question\": RunnablePassthrough()}\n",
    "    | generator_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "monolithic_answer = monolithic_rag_chain_instrumented.invoke(user_query)\n",
    "monolithic_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"               MONOLITHIC RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Context:\")\n",
    "print(retrieved_context_mono + \"\\n\")\n",
    "print(\"Final Answer:\")\n",
    "print(monolithic_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca8fe9",
   "metadata": {},
   "source": [
    "### Running the Sharded RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7904dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SHARDED RAG] Starting run... ---\n",
      "--- [Meta-Retriever] Scattering query to Engineering and Marketing shards in parallel... ---\n",
      "--- [Meta-Retriever] Gathered 4 unique documents from 2 shards. ---\n",
      "--- [Generator] Synthesizing final answer... ---\n",
      "============================================================\n",
      "                 SHARDED RAG SYSTEM OUTPUT\n",
      "============================================================\n",
      "\n",
      "Retrieved Context:\n",
      "[Source: eng-kb] The QuantumLeap V3 processor utilizes a 3nm process node and features a dedicated AI accelerator core with 128 tensor units. API endpoint `/api/v3/status` provides real-time thermal throttling data.\n",
      "\n",
      "[Source: eng-kb] Firmware update v2.1 for the Aura Smart Ring optimizes the photoplethysmography (PPG) sensor algorithm for more accurate sleep stage detection. The update is deployed via the mobile app.\n",
      "\n",
      "[Source: mkt-kb] Press Release: Unveiling the QuantumLeap V3, the AI processor that redefines speed. 'It's a game-changer for creative professionals,' says CEO Jane Doe. Available Q4.\n",
      "\n",
      "[Source: mkt-kb] Product Page: The Aura Smart Ring is your personal wellness companion. Crafted from aerospace-grade titanium, it empowers you to unlock your full potential by understanding your body's signals.\n",
      "\n",
      "Final Answer:\n",
      "Yes, the QuantumLeap V3 is described in a press release as a \"game-changer for creative professionals,\" according to CEO Jane Doe. It utilizes a 3nm process node and includes a dedicated AI accelerator core with 128 tensor units, making it highly efficient for AI-intensive tasks commonly used in creative fields like video editing, design, and real-time rendering.\n",
      "\n",
      "Regarding your question about an API endpoint: Yes, there is an API endpoint, `/api/v3/status`, that provides real-time thermal throttling data for the QuantumLeap V3 processor. This allows developers and system operators to monitor performance conditions and ensure optimal operation.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [SHARDED RAG] Starting run... ---\")\n",
    "start_time = time.time()\n",
    "inputs = {\"question\": user_query}\n",
    "sharded_result = None\n",
    "for output in sharded_rag_app.stream(inputs, stream_mode=\"values\"):\n",
    "    sharded_result = output\n",
    "sharded_time = time.time() - start_time\n",
    "\n",
    "retrieved_context_sharded = format_docs(sharded_result['retrieved_docs'])\n",
    "sharded_answer = sharded_result['final_answer']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                 SHARDED RAG SYSTEM OUTPUT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"Retrieved Context:\")\n",
    "print(retrieved_context_sharded + \"\\n\")\n",
    "print(\"Final Answer:\")\n",
    "print(sharded_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a555de",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4617af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [MONOLITHIC RAG] Starting run... ---\n",
      "--- [Monolithic Retriever] Searching large index... (simulating high latency) ---\n",
      "\n",
      "--- [SHARDED RAG] Starting run... ---\n",
      "--- [Meta-Retriever] Scattering query to Engineering and Marketing shards in parallel... ---\n",
      "--- [Meta-Retriever] Gathered 4 unique documents from 2 shards. ---\n",
      "--- [Generator] Synthesizing final answer... ---\n",
      "\n",
      "============================================================\n",
      "                      ACCURACY & RECALL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "                      PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Monolithic RAG Total Time: 5.39 seconds\n",
      "Sharded RAG Total Time: 3.00 seconds\n",
      "\n",
      "Latency Improvement: 44%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The query contains strong marketing keywords ('game-changer') and a specific technical question ('API status endpoint').\n",
    "user_query = \"I heard the new QuantumLeap V3 is a 'game-changer for creative professionals'. Can you tell me more about it, and is there an API endpoint to check its status?\"\n",
    "\n",
    "# --- Run Monolithic RAG ---\n",
    "print(\"--- [MONOLITHIC RAG] Starting run... ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "monolithic_answer = monolithic_rag_chain.invoke(user_query)\n",
    "monolithic_time = time.time() - start_time\n",
    "\n",
    "# --- Run Sharded RAG ---\n",
    "print(\"\\n--- [SHARDED RAG] Starting run... ---\")\n",
    "start_time = time.time()\n",
    "inputs = {\"question\": user_query}\n",
    "sharded_rag_app.invoke(inputs)\n",
    "sharded_time = time.time() - start_time\n",
    "\n",
    "# --- Final Analysis ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                      ACCURACY & RECALL ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                      PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(f\"Monolithic RAG Total Time: {monolithic_time:.2f} seconds\")\n",
    "print(f\"Sharded RAG Total Time: {sharded_time:.2f} seconds\\n\")\n",
    "latency_improvement = ((monolithic_time - sharded_time) / monolithic_time) * 100\n",
    "print(f\"Latency Improvement: {latency_improvement:.0f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d960dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_experimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
