{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d9f21c-65a8-4fd3-96ea-082ef92b7f9a",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Build an Image Captioning System with Vision Language Models](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecae8a8-8d73-43bb-a5bd-b3b0c7d0ff4c",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268db11b-2455-46e1-9657-1cffe48037ed",
   "metadata": {},
   "source": [
    "In this lab, you’ll explore how to use the Vision model to perform multimodal tasks like image captioning and visual question answering using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4e123-a40c-4100-b0df-b24ca600c1ba",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915cede-ad84-48ad-a476-ef0926d0d74e",
   "metadata": {},
   "source": [
    "## [Introduction](#toc0_)\n",
    "\n",
    "Visual content—like photos, screenshots, or charts—often contains important information that can be hard to interpret at a glance. Wouldn’t it be useful if an AI model could instantly describe what’s in an image, or answer questions about it?\n",
    "\n",
    "In this guided project, we’ll explore how to use a large multimodal language model to do exactly that. You'll use Vision model, integrated with Langchain, to generate text responses based on visual inputs. From scene descriptions to answering specific questions, this model can help turn images into insights.\n",
    "\n",
    "## [What does this guided project do?](#toc0_)\n",
    "\n",
    "This project demonstrates how to:\n",
    "\n",
    "- Load image data from URLs.\n",
    "- Encode those images so they can be processed by a language model.\n",
    "- Use the Vision model to generate text responses based on each image.\n",
    "## [Objectives](#toc0_)\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand how to encode images for LLM-based visual processing.\n",
    "- Use the Vision model to describe or analyze images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17bc1d-e3a3-4452-b77b-a1e6dddc58bd",
   "metadata": {},
   "source": [
    "## [Background](#toc0_)\n",
    "\n",
    "### [What is large language model (LLM)?](#toc0_)\n",
    "\n",
    "[Large language models](https://www.ibm.com/think/topics/large-language-models?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Build+an+Image+Captioning+System+with+watsonx+and+Llama-v1_1745515774) are a category of foundation models that are trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2092c776-dbf3-4b97-b38f-38bf543d5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "gen_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.5,\n",
    "}\n",
    "\n",
    "# Initialize the HuggingFaceEndpoint with your model details\n",
    "hf_endpoint = HuggingFaceEndpoint(\n",
    "    model=model_name,\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=hf_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eeb270-63af-4bc7-a654-2569d68c1900",
   "metadata": {},
   "source": [
    "## <a href=\"#Image-preparation\">Image preparation</a>\n",
    "\n",
    "- Download the image\n",
    "- Display the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0016db2-ac9c-4a5b-876a-ff4e98bdb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_image_1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/5uo16pKhdB1f2Vz7H8Utkg/image-1.png'\n",
    "url_image_2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fsuegY1q_OxKIxNhf6zeYg/image-2.png'\n",
    "url_image_3 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KCh_pM9BVWq_ZdzIBIA9Fw/image-3.png'\n",
    "url_image_4 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VaaYLw52RaykwrE3jpFv7g/image-4.png'\n",
    "\n",
    "image_urls = [url_image_1, url_image_2, url_image_3, url_image_4] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcede01b-f7e2-4cc6-8abf-030e56e4c71c",
   "metadata": {},
   "source": [
    "To gain a better understanding of our data input, let's display the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46e828-782b-42f1-81ef-89ee3cf47ba1",
   "metadata": {},
   "source": [
    "![Image 1](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/5uo16pKhdB1f2Vz7H8Utkg/image-1.png)<figcaption>Image 1</figcaption>\n",
    "\n",
    "![Image 2](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fsuegY1q_OxKIxNhf6zeYg/image-2.png)<figcaption>Image 2</figcaption>\n",
    "\n",
    "![Image 3](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KCh_pM9BVWq_ZdzIBIA9Fw/image-3.png)<figcaption>Image 3</figcaption>\n",
    "\n",
    "![Image 4](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VaaYLw52RaykwrE3jpFv7g/image-4.png)<figcaption>Image 4</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4705b4e-6152-4bf4-b35b-a7f12dca654c",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Encode the image](#toc0_)\n",
    "\n",
    "Encode the image to `base64.b64encode`. Why do you need to encode the image to `base64.b64encode`? JSON is a text-based format and does not support binary data. By encoding the image as a Base64 string, you can embed the image data directly within the JSON structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94af328c-73ed-4d76-892f-8e1c802caa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def encode_images_to_base64(image_urls):\n",
    "    \"\"\"\n",
    "    Downloads and encodes a list of image URLs to base64 strings.\n",
    "\n",
    "    Parameters:\n",
    "    - image_urls (list): A list of image URLs.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of base64-encoded image strings.\n",
    "    \"\"\"\n",
    "    encoded_images = []\n",
    "    for url in image_urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            encoded_image = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "            encoded_images.append(encoded_image)\n",
    "            print(type(encoded_image))\n",
    "        else:\n",
    "            print(f\"Warning: Failed to fetch image from {url} (Status code: {response.status_code})\")\n",
    "            encoded_images.append(None)\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df691d66-6a23-4d95-9989-a5d264dc5c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "encoded_images = encode_images_to_base64(image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62bfc2-b1d0-48ae-b834-c05463a3b244",
   "metadata": {},
   "source": [
    "## <a id='#Multimodal-inference-function'></a>[Multimodal inference function](#toc0_)\n",
    "\n",
    "Next, define a function to generate responses from the model.\n",
    "\n",
    "The `generate_model_response` function is designed to interact with a multimodal AI model that accepts both text and image inputs. This function takes an image, along with a user’s query, and generates a response from the model.\n",
    "\n",
    "#### Function purpose\n",
    "\n",
    "The function sends an image and a query to the AI model and retrieves a description or answer. It combines a text-based prompt and an image to guide the model in generating a concise response.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **`encoded_image`** (`str`): A base64-encoded image string, which allows the model to process the image data.\n",
    "- **`user_query`** (`str`): The user's question about the image, providing context for the model to interpret the image and answer appropriately.\n",
    "- **`assistant_prompt`** (`str`): An optional text prompt to guide the model in responding in a specific way. By default, the prompt is set to: `\"You are a helpful assistant. Answer the following user query in 1 or 2 sentences:\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08b08d42-9546-492a-9078-96d60b56c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(\n",
    "        image_url, \n",
    "        user_query, \n",
    "        assistant_prompt=\"You are a helpful assistant. Answer the following user query in 1 or 2 sentences: \"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Sends an image and a query to the model and retrieves the description or answer.\n",
    "\n",
    "    Parameters:\n",
    "    - image_url (str): URL of the image.\n",
    "    - user_query (str): The user's question about the image.\n",
    "    - assistant_prompt (str): Optional prompt to guide the model's response.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's response for the given image and query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the messages object\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": assistant_prompt + user_query\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Send the request to the model\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return the model's response\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d73d6c-85d5-48b6-ad6e-0f072789c25f",
   "metadata": {},
   "source": [
    "### Steps explained\n",
    "\n",
    "1. **Create the Messages object:**  \n",
    "   The function constructs a list of messages in JSON-like format. This object includes:\n",
    "   - A \"user\" role with a \"content\" array. The content array contains:\n",
    "     - A text field, combining the `assistant_prompt` and the `user_query`.\n",
    "     - An image URL field, which includes a base64-encoded image string. This is essential for sending image data to the model.\n",
    "2. **Send the request to the model:** `response = model.invoke(messages)`\n",
    "\tThe function sends the constructed messages to the model using a chat-based API. The model.chat function is invoked with the messages parameter to generate the model's response.\n",
    "3. **Return the model’s response:** `return response.content`\n",
    "\tThe model’s response is returned as a string, extracted from the response object. Specifically, the function retrieves the content of the first choice in the model's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466f0f33-9436-41b3-9d32-f093b2bd637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for image 1: The photo depicts a bustling urban street lined with tall buildings, featuring a mix of modern and historic architecture. Tall skyscrapers dominate the scene, with one prominently displaying the \"thenoma.com\" advertisement, while the street below is filled with cars, pedestrians, and traffic lights, capturing the dynamic energy of city life./n/n\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Describe the photo\"\n",
    "\n",
    "for i in range(len(image_urls)):\n",
    "    image = image_urls[i]\n",
    "\n",
    "    response = generate_model_response(\n",
    "        image, \n",
    "        user_query\n",
    "    )\n",
    "\n",
    "    # Print the response with a formatted description\n",
    "    print(f\"Description for image {i + 1}: {response}/n/n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4334a-d6a9-4440-a00b-80e54728c743",
   "metadata": {},
   "source": [
    "## <a id='#Object-detection'></a>[Object detection](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5213cc-dc6d-4804-8235-b9f62d8c349b",
   "metadata": {},
   "source": [
    "Now that you have showcased the model's ability to perform image captioning in the previous step, let's ask the model some questions that require object detection. Our system prompt will remain the same as in the previous section. The difference now will be in the user query. Regarding the second image depicting the woman running outdoors, you will be asking the model, \"How many cars are in this image?\". You can comment out the code section for image captioning if you don't want to wait for the response on that part again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How many cars are in this image?\n",
      "Model Response:  There are several cars visible in the image, including taxis and other vehicles, but the exact number cannot be precisely determined due to the perspective and depth of the street.\n"
     ]
    }
   ],
   "source": [
    "image = image_urls[0]\n",
    "\n",
    "user_query = \"How many cars are in this image?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffbd90c6-65dc-4898-9037-525bfaab6061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How many cars are in this image?\n",
      "Model Response:  There is one car visible in the image.\n"
     ]
    }
   ],
   "source": [
    "image = image_urls[1]\n",
    "\n",
    "user_query = \"How many cars are in this image?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ed94c-ff12-4de1-9dd5-f6991b0d3f55",
   "metadata": {},
   "source": [
    "The model correctly identified the singular vehicle in the image. Now, let's inquire about the damage depicted in the image of flooding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78ed882c-3d39-4093-9ffe-4408aadf811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How severe is the damage in this image?\n",
      "Model Response:  The image shows significant flooding that has submerged a property, including a house and some outbuildings, indicating severe damage.\n"
     ]
    }
   ],
   "source": [
    "image = image_urls[2]\n",
    "\n",
    "user_query = \"How severe is the damage in this image?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f0665-81ff-41e7-9251-dd2cc447d96c",
   "metadata": {},
   "source": [
    "This response highlights the value that multimodal AI has for domains like insurance. The model was able to detect the severity of the damage caused to the flooded home. This could be a powerful tool for improving insurance claim processing time.\n",
    "\n",
    "Next, let's ask the model how much sodium content is in the nutrition label image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74fdd47f-29eb-4085-8443-9186460b25ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How much sodium is in this product?\n",
      "Model Response:  This product contains 640mg of sodium per serving.\n"
     ]
    }
   ],
   "source": [
    "image = image_urls[3]\n",
    "\n",
    "user_query = \"How much sodium is in this product?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494efabb-bf31-434f-a0d6-0b3073f4d628",
   "metadata": {},
   "source": [
    "Great! The model was able to discern objects within the images following user queries. We encourage you to try more queries to further demonstrate the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecae848-60c9-45af-8522-b0d305b54eb4",
   "metadata": {},
   "source": [
    "## <a id='#Conclusion'></a>[Conclusion](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55471381-cc0b-4c60-9261-c779c0c4c116",
   "metadata": {},
   "source": [
    "In this lab, you explored the capabilities of Vision model:\n",
    "\n",
    "- Generating detailed image captions\n",
    "\n",
    "- Answering object detection questions (e.g., number of cars in an image)\n",
    "\n",
    "- Assessing visual damage in real-world disaster scenarios\n",
    "\n",
    "- Extracting specific information from product labels\n",
    "\n",
    "This lab not only introduced you to multimodal AI development but also demonstrated how cutting-edge models can turn visual content into actionable insight. Whether you're building apps for enterprise, education, or everyday use, the tools and techniques you’ve learned here are a solid foundation for what's possible with AI today.\n",
    "\n",
    "We encourage you to extend this notebook by asking new questions, uploading your own images, or combining image and text prompts for more advanced reasoning tasks. The future of AI is multimodal—this is your starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e37b10-4d8d-4348-8222-f68ff1d9a2c4",
   "metadata": {},
   "source": [
    "## <a id='#Exercises'></a>[Exercises](#toc0_)\n",
    "\n",
    "Now, let's practice by exploring some other capabilities of this model. Try asking \"How much cholesterol is in this product?\" in the 4th image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baeb3e2d-4ba1-4195-8688-cd9e2129f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How much cholesterol is in this product?\n",
      "Model Response:  The product contains 20mg of cholesterol per serving.\n"
     ]
    }
   ],
   "source": [
    "image = image_urls[3]\n",
    "\n",
    "user_query = \"How much cholesterol is in this product?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2ed02-12c6-4eb5-ac32-f74c5ee63b0b",
   "metadata": {},
   "source": [
    "Try asking \"What is the color of the woman's jacket?\" in the 2nd image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a88d7c5-22cc-4711-bd2a-b7aa26f62881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  What is the color of the woman's jacket?\n",
      "Model Response:  The woman is wearing a bright yellow jacket.\n"
     ]
    }
   ],
   "source": [
    "image = image_urls[1]\n",
    "\n",
    "user_query = \"What is the color of the woman's jacket?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_experimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "prev_pub_hash": "b631c1541313ee4291baea188fd6b6d2f44bf4a8312116e4621f0bec039e5317"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
